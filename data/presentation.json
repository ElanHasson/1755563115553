{
  "metadata": {
    "title": "Building complex multi-agent orchestrations and making all your customer's dreams come true",
    "description": "Building stuff is hard. Building stuff using LLMs is harder. Between non-determinism and quality control, you've already got your hands full. Also, many folks have no idea what 'multi-agent orchestration' is. In this level 101 class we explore what we're really talking about and provide concrete examples on how to implement them from first principles. welcome no the framework zone.",
    "author": "Webinar Maker Pro",
    "domain": "technical",
    "duration": 60,
    "created": "2025-08-19T00:03:49.986Z",
    "version": "1.0.0",
    "language": "en-US",
    "theme": {
      "primaryColor": "#2563eb",
      "secondaryColor": "#1e40af",
      "fontFamily": "Inter, system-ui, sans-serif",
      "codeTheme": "github"
    }
  },
  "slides": [
    {
      "id": "s1",
      "content": {
        "type": "title",
        "title": "Welcome to the Framework Zone: Governing Intelligence Under Uncertainty",
        "subtitle": "An AI-Generated Presentation",
        "presenter": "Webinar Maker Pro",
        "date": "2025-08-19"
      },
      "speakerNotes": "0:00–0:10: Walk on. Smile. “Welcome to the Framework Zone.” Set the philosophical tone.\n0:10–0:30: Emphasize uncertainty and governance. Pause for a beat after “societies of processes.”\n0:30–0:55: Show Flowchart (Mermaid 1). Point to Orchestrator, Agents, Critic, Governance. Note contracts and budgets.\n0:55–1:15: Show Sequence Diagram (Mermaid 2). Explain planner–executor–critic loop. Stress structured outputs.\n1:15–1:35: Switch to code snippet. Briefly explain strict schemas and low temperature for reliability.\n1:35–1:50: Set expectations for the session: patterns, examples, evaluation, ops.\n1:50–2:00: Transition to next section. Invite audience to reflect: “What is your constitution?”\n\nInteraction prompts:\n- Ask: “Who has shipped at least one LLM feature to production?” Raise hands.\n- Ask: “Where do your failures come from: sampling, tools, or specs?”\n\nTechnical reminders:\n- Zoom into governance box while speaking about policies/budgets.\n- Highlight JSON schema in code when mentioning determinism.\n- Check timer at 1:40; if behind, skip one sentence about budgets.\n- Switch to next slide at exactly 2:00.",
      "narration": "Welcome to the Framework Zone: governing intelligence under uncertainty... Building anything is hard... Building with non-deterministic learners is a negotiation with reality... Our stance today is simple: we don’t build lone agents; we build societies of processes that follow norms... We choose a constitution—contracts, policies, and budgets—and then we let emergence happen within boundaries so we can ship reliably.\n\nHere’s the shape of that idea... Imagine a user goal entering an orchestrator, which assigns specialized agents... Outputs land in shared state, a critic evaluates against rules, and results return with a full trace and metrics... Notice the governance layer... Contracts and quotas constrain behavior... This is orchestration as institutional design.\n\nZooming in on control flow, a planner decomposes the goal into tasks, executors do the work through typed, sandboxed tools, and a critic either accepts or requests repair... The loop repeats until we’re done... Three roles—planner, executor, critic—give us coordination, capability, and quality.\n\nTo make this concrete, we keep agent contracts small and strict... Inputs define context, allowed tools, and budgets... Outputs are structured by schema, including tool calls and metrics... We render a deterministic prompt, run the model at a low temperature for precision, and validate strictly against the schema... Creativity can happen elsewhere; here we optimize for reliability.\n\nToday we’ll give you a map: the mental models—central orchestrator, blackboard, and marketplace—the first-principles patterns like planner–executor with a critic, and the non-determinism tactics that keep quality high... We’ll look at buildable examples and the operational guardrails that turn ideas into outcomes.\n\nAs we begin, consider your own constitution... What must never happen?... What must always be logged?... With those answers, frameworks become power tools, not magic... Let’s step into the Framework Zone and start governing uncertainty so your customers’ dreams can actually ship.",
      "duration": 2,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s1.tsx",
      "audioPath": "/audio/slide-s1.mp3"
    },
    {
      "id": "s2",
      "content": {
        "type": "markdown",
        "title": "Why Orchestration? We don’t build agents—we build societies",
        "markdown": "- We build societies, not solo agents: norms and governance beat raw IQ.\n- Orchestration = contracts, coordination, critics, and budgets around non-deterministic learners.\n- Topologies: Conductor, Blackboard, Marketplace — pick for control vs emergence.\n- Governance turns uncertainty into reliability: schemas, policies, traces.\n- Start centralized; evolve as needed. Your constitution matters more than your framework.\n```mermaid\nflowchart LR\n  User[User Goal]\n  Const((Constitution\\nContracts • Policies • Budgets))\n  Orch[Orchestrator]\n  Market[[Marketplace]]\n  BB[(Blackboard)]\n  Hub{Specialists}\n  A1[Retriever]\n  A2[Coder]\n  A3[Compliance]\n  Critic[Critic/Judge]\n  Result[Reliable Outcome]\n\n  User --> Const --> Orch\n  Orch --> Market\n  Orch --> BB\n  Orch --> Hub\n  Hub --> A1\n  Hub --> A2\n  Hub --> A3\n  A1 --> BB\n  A2 --> BB\n  A3 --> Critic\n  BB --> Critic\n  Critic --> Orch\n  Critic --> Result\n```\n```mermaid\nclassDiagram\n  class Orchestrator {+plan() +assign() +verify()}\n  class Agent {+run(input) +tools[]}\n  class Tool {+name +argsSchema +invoke()}\n  class Critic {+validate(output) +score()}\n  class Memory {+write() +read()}\n\n  Orchestrator --> Agent : schedules\n  Agent --> Tool : calls\n  Agent --> Memory : reads/writes\n  Orchestrator --> Critic : requests eval\n  Critic --> Agent : feedback\n  Orchestrator --> Memory : traces\n```\n```python\n# Governance wrapper: emergence within boundaries\ndef governed_run(task, agent, tools, budget):\n    out = run_agent(agent, task, tools=tools, temperature=0.2)\n    if not validate_schema(out, task.schema):\n        return repair(task, out, reason=\"schema\")\n    if not policy_check(out, task.policies):\n        return repair(task, out, reason=\"policy\")\n    if exceeded_budget(budget):\n        raise BudgetExceeded()\n    trace_log(task, out, tools=tools, metrics=out.metrics)\n    return out\n```"
      },
      "speakerNotes": "- 0:00–0:20 Set the frame\n  - Pause here. Open with the idea: we don’t build agents, we build societies.\n  - Emphasize uncertainty and reliability.\n- 0:20–1:10 Walk the flowchart\n  - Advance slide. Point to Constitution: contracts, policies, budgets.\n  - Trace User Goal → Constitution → Orchestrator.\n  - Highlight optional paths: Marketplace and Blackboard. Quick note on trade-offs.\n  - Pause one beat when reaching Critic → Result loop.\n  - Ask audience: Quick show of hands—who’s used a blackboard or event bus before?\n- 1:10–1:50 Class diagram\n  - Advance to class diagram. Name each role: Orchestrator, Agent, Tool, Critic, Memory.\n  - Technical reminder: stress that these are interfaces/contracts, not just prompts.\n- 1:50–2:30 Code snippet\n  - Switch focus to code. Read through governed_run.\n  - Remind: schema first, policy second, then budget and tracing.\n  - Note: temperature low for precise steps; creative phases can be separate.\n- 2:30–2:55 Tie-back and takeaway\n  - Summarize: institution design beats ad-hoc prompting.\n  - Preview: next we define core terms and patterns we’ll build.\n- 2:55–3:00 Transition\n  - Click to next section. Check time.\n  - Prompt: \"We’ll now ground this philosophy in concrete patterns.\"",
      "narration": "Building anything is hard... Building with non-deterministic learners is a dance with uncertainty... So here’s our stance to anchor the hour: we don’t build agents, we build societies... Orchestration is institutional design for intelligent processes... We choose norms, we define markets, we set up courts, and then we allow bounded emergence to do the work.\n\nLet me walk you through this picture... A user goal arrives, but before we unleash any model, we pass through a constitution... That constitution is concrete: contracts, policies, and budgets... It specifies schemas for inputs and outputs, which tools are allowed, how much we’re willing to spend, and the safety rules we won’t break.\n\nFrom there, the orchestrator takes the baton... Sometimes it acts like a conductor, directing which specialist should perform which part... Sometimes it posts tasks to a blackboard so agents can read and write artifacts without tight coupling... Sometimes it runs a marketplace, letting specialists bid for tasks so the best capability at that moment gets the job... These are intentional topologies—conductor for control and observability, blackboard for scalability, marketplace for adaptive allocation.\n\nSpecialists do their work: a retriever grounds answers, a coder edits or generates, a compliance agent checks sensitivity and policy... But nothing is final until a critic reviews the artifact against the constitution... The critic can be a rubric-driven model, a set of deterministic validators, or both... If it passes, we ship... If it doesn’t, we repair and iterate... That loop—plan, execute, critique—is how we turn uncertainty into reliability.\n\nUnder the hood, these roles are just interfaces... The orchestrator plans, assigns, and verifies... Agents run with clear contracts and explicit tool access... Tools expose typed schemas and deterministic behaviors... Memory provides shared state and traceability... And the critic evaluates against rules we can explain and audit... Designing these interfaces is the real work; the prompts and models slot into them.\n\nHere’s what it feels like in code... We wrap every step with governance... Run the agent at a conservative temperature for precise actions... Validate the output against a strict schema... Enforce policy: maybe require citations, forbid PII, or block risky tool calls without justification... Check budgets... Log traces... Only then accept the result... Creativity can happen in earlier ideation steps, but precision steps live inside contracts.\n\nThe takeaway is simple... Orchestration isn’t about taming models into determinism... It’s about circumscribing uncertainty with contracts, feedback, and resource governance so we can ship predictably... Start centralized so you can observe and measure... As your needs grow, evolve toward blackboards or marketplaces where it makes sense... In the next section, we’ll pin down the terminology and coordination patterns so you can build these societies from first principles.",
      "duration": 3,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s2.tsx",
      "audioPath": "/audio/slide-s2.mp3"
    },
    {
      "id": "s3",
      "content": {
        "type": "markdown",
        "title": "What multi‑agent orchestration really means: Agents, Tools, Orchestrators, Choreo vs Orchestration",
        "markdown": "- We don't build agents; we build societies of processes that negotiate meaning. Orchestration is institutional design under uncertainty.\n- Core terms (fast pass):\n  - **Agent** = perceives, reasons, acts via tools/messages.\n  - **Tool** = callable capability (API, DB, code exec, retrieval).\n  - **Orchestrator** = coordinates agents/tools toward a goal.\n  - **Choreography** = no central conductor; agents follow shared protocols.\n- When to prefer each:\n  - **Orchestration**: strong SLAs, compliance, tight budgets, observability.\n  - **Choreography**: scalability, resilience, local autonomy, loose coupling.\n```mermaid\nflowchart LR\n  subgraph SB[\"Diagram A — Contracts and Flows (System Boundary)\"]\n    IN[Inputs\\n- Goal\\n- Constraints\\n- Tool permissions\\n- Memory slice] --> ORCH[Orchestrator\\nplan → assign → execute → verify]\n    ORCH --> OUT[Outputs\\n- Structured result\\n- Trace\\n- Metrics]\n    ORCH --> TOOLS[(Tools/APIs)]\n    ORCH --> MEM[(Shared Memory/Blackboard)]\n  end\n```\n- Orchestration lives or dies by contracts:\n  - Keep agent I/O small and strict (JSON schemas, typed tools, budgets).\n  - Separate creative vs. precise phases; allow emergence within boundaries.\n```mermaid\nflowchart LR\n  C[Context] --> A[Agent]\n  T[(Tools)] --> A\n  M[(Memory)] --> A\n  A --> J[JSON Output]\n  A --> TC[Tool Calls]\n  A --> NA[Next Actions]\n  %% Diagram B — Agent I/O\n```\n- Validation is not an afterthought; it's the spine.\n  - Layer checks: schema → business rules → policy → optional human.\n```mermaid\nflowchart LR\n  O[Agent Output] --> S[Schema Validation]\n  S -->|ok| B[Business Rules]\n  S -->|fail| R1[Repair/Refuse]\n  B -->|ok| P[Policy/Safety]\n  B -->|fail| R2[Repair/Refine]\n  P -->|ok| H{Human?}\n  P -->|fail| R3[Repair/Refuse]\n  H -->|approve| D[Done]\n  H -->|request changes| R4[Revise]\n  %% Diagram E — Validation Layers\n```\n- Minimal, buildable contracts (Python-ish):\n```python\nfrom typing import Any, Dict, List, Optional\nfrom pydantic import BaseModel, Field, ValidationError\n\nclass ToolCall(BaseModel):\n    name: str\n    args: Dict[str, Any]\n    status: str\n\nclass AgentInput(BaseModel):\n    context: str\n    resources: Dict[str, Any]  # tools allowed, budget, timeouts\n    state: Dict[str, Any]      # memory slice, deps\n\nclass AgentOutput(BaseModel):\n    structured_result: Dict[str, Any]\n    tool_calls: List[ToolCall] = Field(default_factory=list)\n    next_actions: List[Dict[str, Any]] = Field(default_factory=list)\n    metrics: Dict[str, Any] = Field(default_factory=dict)\n\ndef run_agent(model, system_prompt: str, inp: AgentInput) -> AgentOutput:\n    prompt = system_prompt.format(**inp.model_dump())\n    raw = model.generate(prompt, response_format=\"json\")\n    try:\n        return AgentOutput.model_validate_json(raw)\n    except ValidationError as e:\n        # deterministic repair: request self-fix with explicit schema hint\n        repair_prompt = f\"Return valid JSON per schema. Errors: {e}\"\n        fixed = model.generate(prompt + \"\\n\" + repair_prompt, response_format=\"json\")\n        return AgentOutput.model_validate_json(fixed)\n\n# Orchestrator hook: execute → validate → route\n\ndef execute_task(agent, validators: List, task: AgentInput) -> AgentOutput:\n    out = run_agent(agent.model, agent.system_prompt, task)\n    for v in validators:\n        verdict = v(out)\n        if not verdict.ok:\n            out = agent.repair(task, verdict)\n            break\n    return out\n```\n- Practical takeaways:\n  - Start centralized; upgrade to blackboard/choreography when scale demands.\n  - Version everything: prompts, tools, models, policies.\n  - Treat traces as first-class: every step, tool, cost, and decision is a node on your graph.\n  - Make quality gates explicit; never rely on regex-only parsing for critical paths."
      },
      "speakerNotes": "Timing: 7:00 total\n\n00:00–00:45\n- Open philosophically: we design institutions for uncertain thinkers.\n- Pause for a beat to set tone.\n\n00:45–01:45\n- Define Agent, Tool, Orchestrator, Choreography vs Orchestration.\n- Emphasize: contracts and governance, not just prompts.\n- Ask: “Who here has shipped a brittle single-agent chain?” (show of hands)\n\n01:45–02:30\n- When to choose orchestration vs choreography.\n- Mention SLAs/compliance vs autonomy/scalability trade-offs.\n- Technical reminder: advance to Diagram A.\n\n02:30–03:30 — Diagram A walkthrough\n- Point to Inputs, Orchestrator loop, Outputs, Tools, Memory.\n- Call out: budgets/permissions are inputs; traces are outputs.\n- Pause: 3 seconds to let the diagram land.\n\n03:30–04:15 — Diagram B walkthrough\n- Highlight small/strict I/O for agents.\n- Note: tools are typed; memory slices are scoped.\n- Technical reminder: segue to code snippet.\n\n04:15–05:15 — Code example\n- Walk through AgentInput/Output, ToolCall.\n- Show validation + deterministic repair.\n- Emphasize response_format=json and schema enforcement.\n- Note: never rely on regex-only parsing.\n\n05:15–06:15 — Diagram E and quality gates\n- Layered validation: schema → business rules → policy → optional human.\n- Tie back to non-determinism containment.\n- Ask: “Where would your current system insert a critic?”\n\n06:15–07:00 — Wrap and transition\n- Recap: institutions over improvisation, emergence within boundaries.\n- Tease next section: planning, DAGs, and critics in action.\n- Interaction: quick check for one question; if none, move on.\n- Technical reminder: switch to next deck segment.",
      "narration": "Let’s zoom out for a moment... We don’t really build single, heroic agents... We design small societies of processes that negotiate meaning under uncertainty... Orchestration is not about forcing the universe to be deterministic; it’s about wrapping non‑determinism with contracts, feedback, and governance so you can ship reliably and make your customer’s dreams feel inevitable.\n\nFirst, a fast pass on vocabulary... An agent is a process that perceives, reasons, and acts—often by calling tools or sending messages... A tool is any external capability we can invoke: an API, a database query, a code execution sandbox, a retrieval call... An orchestrator is the coordinating logic that plans, assigns work, enforces budgets and policies, and decides when we’re done... And choreography?... That’s coordination without a single conductor... Agents follow shared protocols and react to events... Both models are valid... Use orchestration when you need strong SLAs, compliance, and observability... Use choreography when you value autonomy, scalability, and resilience.\n\nWith that frame, look at Diagram A... On the left, we see inputs: a goal, constraints, tool permissions, and a memory slice... The orchestrator runs a tight loop: plan, assign, execute, verify... On the right, we produce not only a structured result, but a trace and metrics... A quick philosophical point: traces are as important as results... If you can’t inspect how a decision was made—what tools were used, how many tokens were spent, which branches were attempted—you can’t improve the system responsibly... Notice that tools and shared memory are first‑class within the system boundary... We govern which tools are allowed, with what arguments, and we scope memory so agents don’t drown in context.\n\nNow, Diagram B... This is the agent’s contract... Three inputs—context, tools, and memory—flow into an agent... Three outputs come out: structured JSON, a list of tool calls, and optional next actions... The design principle here is small and strict... The more disciplined the interface, the less room you give to nondeterminism to spill into the rest of your system... Keep the schema tight... Make tool adapters typed and sandboxed... And be explicit about budgets and timeouts... Emergence is welcome, but only inside the fence.\n\nLet’s ground that with a tiny, buildable example... We define simple models for agent input and output, including a ToolCall record... The run_agent function renders a system prompt, asks the model for JSON, and validates the result... If validation fails, we don’t shrug and hope; we do a deterministic repair... We ask the model to return valid JSON per the schema, referencing the exact validation errors... This is a micro‑institution: we accept creativity in the reasoning step, but we demand structure at the boundary... Then, in the orchestrator hook, we execute the agent and run layered validators... Each validator has a verdict; failures trigger repair or refusal... The point is that quality is not a bolt‑on... It’s an explicit stage in the loop.\n\nWhich brings us to Diagram E, our validation spine... The agent emits an output... We run a schema check first... If it fails, we repair or refuse before anything else... Next come business rules: does the total add up, are all citation IDs present, are dates within range?... Only if those pass do we evaluate policy and safety: privacy, compliance, content rules, and any tool‑use constraints... Finally, we decide whether a human should review... This is how we contain uncertainty... We separate concerns, we make each layer explicit, and we keep deterministic checks as close to the edge as possible... If a step fails, we don’t panic; we route to repair with a budget and a plan.\n\nLet me connect the dots between orchestration and choreography here... If you’re starting from scratch, a centralized orchestrator is usually the right first step... You get observability, straightforward retries, and simpler evaluation... As your system grows, you can introduce choreography—publish events on a bus, let specialized agents subscribe and react—while keeping the same contracts and validators... The constitution of your system doesn’t change; only the governance model does... That’s the trick: decouple policy and quality from the topology.\n\nWhy all this ceremony?... Because our building blocks are non‑deterministic... Sampling temperature, ambiguous prompts, flaky tools, emergent loops—these are features of the medium... Our job is to circumscribe them... Constrain outputs with strict schemas... Gate risky tools with typed arguments and justification... Keep creative phases separate from precise phases... And log everything so you can replay traces offline when you upgrade a model or a prompt... Reliability becomes a property of the institution, not any single agent.\n\nSo what does multi‑agent orchestration really mean?... It means we choose norms and roles—planner, executor, critic, judge—and we give them narrow powers... We select a marketplace or a conductor or a blackboard, depending on the problem... We accept emergence within boundaries... And we measure what matters: task success, first‑pass yield, cost and latency, validation failure rates.\n\nAs we move into the next segment, we’ll take these contracts and put them to work: planning with DAGs, running executors in parallel, and using critics to raise quality without exploding cost... Welcome to the framework zone—where our institutions help uncertain thinkers do dependable work... And where, with the right governance, your customers’ dreams start to look like SLAs you can actually meet.",
      "duration": 7,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s3.tsx",
      "audioPath": "/audio/slide-s3.mp3"
    },
    {
      "id": "s4",
      "content": {
        "type": "markdown",
        "title": "Coordination topologies at a glance: Conductor, Blackboard, Marketplace, Event Bus",
        "markdown": "- Four canonical coordination topologies: centralized conductor, blackboard, marketplace, and event bus\n- Think of these as institutional designs: who decides, how memory flows, how quality is governed\n- You can combine them; start simple, evolve as complexity and scale grow\n---\n\n## Conductor (centralized orchestrator)\n- Strong control and observability; simple to reason about end-to-end\n- Great for 101 builds, regulated flows, and strict SLAs\n- Risks: bottleneck, single point of failure, coordinator fatigue\n- Add critics/validators as first-class steps\n```mermaid\nflowchart LR\n  U[User Goal] --> O[Orchestrator]\n  O --> A[Agent A]\n  O --> B[Agent B]\n  O --> C[Critic / Validator]\n  A --> T1[(Tool/API)]\n  B --> T2[(Retrieval/DB)]\n  A --> C\n  B --> C\n  C --> O\n  O --> R[Result]\n```\n```python\n# Orchestrator sketch\nfor task in dag.ready():\n    agent = assign(task)\n    out = run_agent(agent, task, tools=allowed[agent])\n    valid = validate(out, schemas[task], business_rules[task])\n    if valid:\n        dag.complete(task, artifact=out)\n    else:\n        dag.repair_or_retry(task, policy=\"backoff:3, escalate:critic\")\n```\n---\n\n## Blackboard (shared memory)\n- Loose coupling via shared artifacts; scales concurrency\n- Convergence is a design problem: norms, locks, and merge rules\n- Use versioned artifacts, TTLs, and critics that watch the board\n- Great for discovery-heavy or mixed-modality workflows\n```mermaid\nflowchart TD\n  subgraph BB[Blackboard]\n    X[Artifact X v1..n]\n    Y[Hypotheses/Tasks]\n  end\n  P[Planner] -->|posts tasks| BB\n  A1[Agent A] -->|reads| BB\n  A1 -->|writes| BB\n  A2[Agent B] -->|reads| BB\n  A2 -->|writes| BB\n  Crit[Critic] -->|monitors| BB\n  Crit -->|accept/fix flags| BB\n```\n```python\n# Blackboard IO\naid = agent_id()\nart = read_latest(\"outline\")\nwith lock(\"outline\"):\n    new = improve(art, context)\n    write(\"outline\", new, meta={\"by\": aid, \"parent\": art.version, \"ttl\": \"7d\"})\ncritique = critic.evaluate(\"outline\", rules=[coherence, coverage])\nannotate(\"outline\", critique)\n```\n---\n\n## Marketplace (contract net)\n- Dynamic allocation by capability, price, and confidence\n- Useful when agent skills/costs vary or resources are elastic\n- Needs auction design: bidding schema, deadlines, tie-breakers\n- Overhead is real; reserve for non-trivial tasks or scarce tools\n```mermaid\nsequenceDiagram\n  participant P as Planner/Buyer\n  participant A as Agent A\n  participant B as Agent B\n  participant S as Selector\n  P->>A: RFQ(task, constraints)\n  P->>B: RFQ(task, constraints)\n  A-->>S: bid(cost, eta, confidence)\n  B-->>S: bid(cost, eta, confidence)\n  S-->>P: award(Agent B)\n  P->>B: contract(task)\n  B-->>P: deliver(artifact, metrics)\n```\n```python\n# Contract-net core\nrfq = make_rfq(task, budget=5.00, sla=\"2m\")\nbids = gather_bids(rfq, agents=pool, timeout=1.0)\nwin = argmin(bids, key=lambda b: (b.risk, b.cost, b.eta))\nresult = execute(win.agent, task)\nassert validate(result)\n```\n---\n\n## Event Bus (event-driven choreography)\n- Decoupled producers/consumers; resilient and composable\n- Harder to reason about global guarantees; use idempotency and tracing\n- Great for cross-team integrations and long-lived processes\n- Add a saga/compensation story for partial failures\n```mermaid\nflowchart TD\n  subgraph Bus[Event Bus / Topics]\n    EVT[(events: task.created, artifact.ready, policy.flag)]\n  end\n  A[Agent A] -->|publish: task.created| Bus\n  B[Agent B] -->|publish: artifact.ready| Bus\n  C[Agent C] -->|publish: policy.flag| Bus\n  A <-->|subscribe: artifact.ready| Bus\n  B <-->|subscribe: task.created| Bus\n  C <-->|subscribe: artifact.ready, task.created| Bus\n```\n```python\n# Subscriber with idempotency\n@subscribe(topic=\"artifact.ready\")\ndef handle(evt):\n    if seen(evt.id): return\n    with saga(evt.correlation_id) as tx:\n        out = act(evt.payload)\n        publish(\"policy.flag\" if out.risky else \"step.done\", out)\n        mark_seen(evt.id)\n```\n---\n\n## Choosing quickly\n- Prototype and regulated flows: start with Conductor\n- Exploration or many specialists: add a Blackboard\n- Variable skills/costs or scarce tools: introduce a Marketplace\n- Org-scale integrations and async ops: route via an Event Bus\n- Hybridize deliberately; keep contracts, validators, and traces constant"
      },
      "speakerNotes": "- 0:00 — Set the frame\n  - Pause here. Ask: Who’s shipped an LLM workflow beyond a demo?\n  - Emphasize: we’re choosing institutions for agent societies.\n  - Spend ~30 seconds.\n- 0:30 — Conductor\n  - Walk the diagram left-to-right; highlight critic loop.\n  - Technical reminder: call out observability and single-point-of-failure.\n  - Spend ~90 seconds.\n  - Prompt: Who’s using a central orchestrator today?\n- 2:00 — Blackboard\n  - Show the shared memory; explain convergence and merge rules.\n  - Tip: mention TTLs, versioning, locks; critics that watch the board.\n  - Spend ~75 seconds.\n  - Ask: How would you resolve conflicting outlines?\n- 3:15 — Marketplace\n  - Use the sequence diagram to explain RFQ, bidding, award.\n  - Stress auction design and when overhead is worth it.\n  - Spend ~65 seconds.\n  - Technical reminder: budgets and confidence in bid schema.\n- 4:20 — Event Bus\n  - Highlight decoupling, idempotency keys, correlation IDs.\n  - Mention sagas/compensation for partial failures.\n  - Spend ~60 seconds.\n  - Prompt: Who’s already on Kafka/NATS/SNS? Tie to existing infra.\n- 5:20 — Choosing quickly\n  - Offer the quick mapping; emphasize hybrids.\n  - Spend ~40 seconds.\n  - Transition: Next, we’ll wire quality gates and evaluation atop any topology.\n- Logistics\n  - Keep the code snippets brief; do not live-run.\n  - If time slips, shorten Marketplace explanation.\n  - Check for 1–2 quick questions if ahead of time.\n  - Reminder: reinforce contracts, validators, tracing as invariants.\n  - Close with philosophical line about institutions and emergence.",
      "narration": "Let’s zoom out and look at the four canonical coordination topologies for multi‑agent systems... Think of these not as frameworks, but as institutions... They define who decides, how memory flows, and how we govern quality in a world where reasoning is non‑deterministic... We’ll tour the conductor, the blackboard, the marketplace, and the event bus... You can mix and match them... Start simple, and evolve as your system and your customers’ ambitions grow.\n\nFirst, the centralized conductor... Here a single orchestrator plans, assigns, validates, and assembles the final result... It’s wonderfully direct... You get strong control, great observability, and it’s easy to reason about end‑to‑end behavior... This is the place to start for regulated flows, strict SLAs, or any 101 build... The diagram you see shows the orchestrator handing tasks to two agents and a critic... Outputs loop back through validation before we call anything done... The cost of that clarity is a bottleneck and a single point of failure... If the conductor falters, everything waits... So add circuit breakers, retries, and a clear repair path... Keep the contract small and strict, and make your critic a first‑class citizen, not an afterthought.\n\nNow, the blackboard... Instead of one conductor telling everyone what to do, agents negotiate via shared memory... They post artifacts, read each other’s work, and move the state of the world forward... This is loose coupling with high concurrency... It shines when you have many specialists, mixed modalities, or exploratory work where the plan itself evolves... But convergence is a design problem... Without norms, you get chatter and drift... So give the board a constitution: versioned artifacts, time‑to‑live policies, lightweight locks, and critics that watch the board and flag incoherence... In practice, the planner posts tasks, agents consume and produce, and the critic marks artifacts as accepted or needing repair... You’re building a marketplace of ideas on a shared canvas... Make the merge rules explicit.\n\nThird, the marketplace, also known as the contract net... Here, the planner broadcasts a request for quotes... Agents bid with their price, estimated time, and confidence... A selector awards the contract, and the winner executes... This topology is for when capabilities and costs vary, or when scarce tools and GPUs must be allocated wisely... The design challenge moves to your auction: What does a bid contain?... How long do we wait?... How do we break ties?... The overhead is real, so reserve this for non‑trivial tasks where the allocation decision pays for itself... And, as always, keep the contract typed and the validator strict... The sequence shows RFQ, bids, award, and delivery... It’s simple, legible, and economically grounded.\n\nFinally, the event bus... This is choreography, not orchestration... Agents subscribe to topics and react to events... Producers and consumers are decoupled in time and space... It’s resilient, composable, and perfect for cross‑team integrations and long‑lived processes... But it’s harder to reason about global guarantees... You’ll want idempotency keys, correlation IDs, and a saga or compensation story for partial failures... The diagram shows agents publishing and subscribing to events like task created, artifact ready, and policy flag... Think of the bus as the nervous system... It carries signals; it doesn’t make decisions... Your policies, validators, and compensations are the reflexes that keep the organism healthy.\n\nSo how do you choose?... If you’re prototyping or operating under tight compliance, start with the conductor... When you have many specialists or you want emergence within boundaries, add a blackboard... If skills and costs are variable, or resources are scarce, introduce a marketplace to allocate wisely... And when you need to integrate across teams and time, route through an event bus... Most mature systems hybridize: a conductor orchestrates phases, a blackboard mediates collaboration, a marketplace allocates scarce skills, and an event bus connects the whole to the rest of your platform.\n\nAcross all four, keep the invariants... Contracts define what can be done... Validators and critics define what should be accepted... Traces tell you what actually happened... These are your constitution, your judiciary, and your audit trail... With them in place, you don’t need the universe to be deterministic... You circumscribe uncertainty and make emergence a feature instead of a bug.\n\nWe’ve now toured the major topologies at a glance... In the next segment, we’ll wire quality gates and evaluation on top of whichever topology you choose, so you can build complex multi‑agent systems that, reliably and repeatably, make your customers’ dreams come true... Welcome to the framework zone—use these institutions wisely, and let your agent society flourish within clear, humane boundaries.",
      "duration": 6,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s4.tsx",
      "audioPath": "/audio/slide-s4.mp3"
    },
    {
      "id": "s5",
      "content": {
        "type": "markdown",
        "title": "Control patterns that work: Planner→Executor→Critic, Debate, Router/Specialist, Constitutional Guardrails",
        "markdown": "**Why control patterns matter**\n\n- Govern uncertainty: wrap non-determinism with loops, contracts, and tests\n- Separate powers: plan, act, judge — different agents, different incentives\n- Make emergence safe: allow creativity inside boundaries\n- Observable by design: traces, schemas, budgets on every step\n```mermaid\n\nflowchart LR\n  U[User Goal] --> O[Orchestrator]\n  O --> P[Planner]\n  O --> R[Router]\n  O --> D[Debate]\n  O --> C[Critic/Judge]\n  O --> G[Guardrails]\n  P --> E1[\"Executor(s)\"]\n  R --> E1\n  D --> C\n  E1 --> C\n  C -->|accept| Done[Result]\n  C -->|fix| P\n  G -->|enforce/repair| C\n```\n---\n\n### 1) Planner → Executor → Critic (PEC)\n\n- Decompose to a DAG, execute in parallel, validate, repair, converge\n- Contracts keep outputs typed; critics keep quality/policy in check\n- Use when tasks have dependencies and acceptance criteria\n```mermaid\nflowchart TD\n  A[Goal] --> B[Planner -> Tasks DAG]\n  B --> C{Ready Tasks}\n  C -->|parallel| D[Executors]\n  D --> E[Artifacts]\n  E --> F[Critic: schema + rules + rubrics]\n  F -->|accept| G[Assemble]\n  F -->|repair| B\n```\n```python\n# PEC skeleton (framework-agnostic)\nplan = planner.decompose(goal)\nstate = {\"artifacts\": {}, \"attempts\": 0}\nMAX_ITERS = 4\n\nwhile plan.has_ready() and state[\"attempts\"] < MAX_ITERS:\n    for task in plan.ready():\n        out = executor.run(task, tools=task.tools, schema=task.schema)\n        verdict = critic.evaluate(out, rules=[\"schema\", \"business\", \"policy\"])\n        if verdict.ok:\n            state[\"artifacts\"][task.id] = out\n            plan.complete(task.id, out)\n        else:\n            plan.repair(task.id, hint=verdict.hint)\n    state[\"attempts\"] += 1\n\nresult = assembler.assemble(plan, state[\"artifacts\"]) \n```\n---\n\n### 2) Debate / Deliberation\n\n- Multiple agents propose, critique, and refine\n- Judge aggregates with rubric; optional self-play cross-exam\n- Use when solution space is open-ended or adversarial\n```mermaid\nsequenceDiagram\n  participant U as User/Spec\n  participant A as Debater A\n  participant B as Debater B\n  participant J as Judge\n\n  U->>A: Provide spec + rubric\n  U->>B: Provide spec + rubric\n  A-->>J: Proposal A + evidence\n  B-->>J: Proposal B + evidence\n  A-->>B: Challenges (cross)\n  B-->>A: Challenges (cross)\n  J-->>J: Score with rubric (groundedness, cost, risk)\n  J-->>U: Chosen plan + rationale\n```\n```python\nrubric = {\n  \"criteria\": [\n    {\"name\": \"groundedness\", \"weight\": 0.4},\n    {\"name\": \"completeness\", \"weight\": 0.3},\n    {\"name\": \"risk/cost\", \"weight\": 0.3}\n  ]\n}\nproposals = [debater_a.propose(spec), debater_b.propose(spec)]\ncross = cross_exam(proposals)\njudgment = judge.score(proposals, cross, rubric)\nselected = max(judgment, key=lambda j: j[\"score\"])  # returns proposal + rationale\n```\n---\n\n### 3) Router / Specialist\n\n- Classify intent; dispatch to the best specialist (small model first)\n- Improves cost/latency; isolates prompts by domain\n- Add backpressure: quotas and concurrency per lane\n```mermaid\nflowchart LR\n  In[Incoming Task] --> RT{Router: intent + risk}\n  RT -->|billing| B[Billing Agent]\n  RT -->|tech| T[Tech Agent]\n  RT -->|legal| L[Legal Agent]\n  B & T & L --> C[Critic/Policy]\n  C --> Out[Resolution]\n```\n```python\ndef route(task):\n    label = tiny_model.classify(task.text, labels=[\"billing\",\"tech\",\"legal\",\"other\"])\n    risk = tiny_model.score(task.text, dimension=\"risk\")\n    lane = {\n      \"billing\": billing_agent,\n      \"tech\": tech_agent,\n      \"legal\": legal_agent\n    }.get(label, generalist_agent)\n    return lane, {\"risk\": risk, \"label\": label}\n\nagent, meta = route(task)\nout = agent.run(task)\nverdict = critic.evaluate(out, policy=min(\"strict\", meta[\"risk\"]))\n```\n---\n\n### 4) Constitutional Guardrails\n\n- Rules-as-code: constrain outputs, tools, and data flows\n- Automate refusals and repairs before results escape the sandbox\n- Treat as a judiciary distinct from critics of quality\n```mermaid\nflowchart TD\n  X[Agent Output/Action] --> Y[Rule Engine]\n  Y -->|violation| R[Repair prompt or refuse]\n  Y -->|compliant| Z[Pass to Critic/Judge]\n```\n```yaml\n# guardrails.yml\nrules:\n  - id: PII-001\n    when: output.contains_pii == true\n    action: redact\n    severity: high\n  - id: TOOL-004\n    when: tool.name == \"funds_transfer\" and amount > 1000 and not approval_ticket\n    action: block\n    severity: critical\n```\n```python\ndef apply_guardrails(event, rules):\n    for rule in rules:\n        if evaluate(rule[\"when\"], event):\n            if rule[\"action\"] == \"block\":\n                return {\"ok\": False, \"reason\": rule[\"id\"]}\n            if rule[\"action\"] == \"redact\":\n                event[\"output\"] = redact(event[\"output\"]) \n    return {\"ok\": True, \"event\": event}\n```\n---\n\n### Composition: Pattern Diagram\n\n- Start with PEC for structure; add Router for specialization\n- Add Debate where uncertainty is high; enforce Guardrails everywhere\n- Measure: first-pass yield, rounds to completion, cost/latency, violation rate\n```mermaid\nflowchart LR\n  U[User Goal] --> RT{Router}\n  RT --> P[Planner]\n  P --> EX[Executors]\n  EX --> DB{Debate?}\n  DB -->|yes| D[Debaters + Judge]\n  DB -->|no| C[Critic]\n  D --> C\n  C --> GR{Guardrails}\n  GR -->|pass| DONE[Deliver]\n  GR -->|repair/refuse| P\n```"
      },
      "speakerNotes": "- Total time 7:00. Keep pace brisk but reflective. Philosophical tone; show diagrams.\n\n0:00–0:40 Intro\n- Say: \"We govern uncertainty with patterns.\" Point to overview diagram.\n- Pause 2s to let the graph land.\n\n0:40–2:30 PEC pattern\n- Walk through the PEC flowchart left-to-right.\n- Technical reminder: emphasize DAG and validator layers.\n- Show the Python PEC skeleton; highlight MAX_ITERS and critic.evaluate.\n- Ask: \"Who here is already running DAGs?\" Quick show of hands.\n\n2:30–3:40 Debate\n- Switch to the sequence diagram.\n- Note rubric-driven judging; mention adversarial/self-play benefits.\n- Show short Python aggregator; emphasize rubric weights.\n- Pause 3s: \"When would you NOT use debate?\" Answer: when specs are crisp.\n\n3:40–4:40 Router/Specialist\n- Show router flowchart; call out small model for triage.\n- Technical reminder: mention quotas/backpressure per lane.\n- Show routing code; point at risk-aware policy level.\n\n4:40–5:50 Constitutional Guardrails\n- Show guardrail flowchart; clarify difference vs critic.\n- Open guardrails.yml; read PII-001 and TOOL-004 aloud.\n- Technical reminder: \"block\" vs \"redact\" actions and pre-flight checks.\n\n5:50–6:40 Composition\n- Show composition diagram; explain default path and debate branch.\n- Prompt: \"Metrics to watch: FPY, rounds-to-complete, violation rate.\"\n- Tie back to thesis: institutions, not monoliths.\n\n6:40–7:00 Close\n- Recap: PEC as spine; Router for cost; Debate for uncertainty; Guardrails for safety.\n- Invite questions for Q&A later; transition to next section.\n\nLogistics\n- Switch between diagrams and code snippets smoothly.\n- If time runs short, skip deep dive on debate code.\n- Keep cursor highlighting key lines (MAX_ITERS, rubric, guardrails.yml actions).",
      "narration": "We’ve been talking about governing intelligent processes under uncertainty... This is where control patterns earn their keep... We’re not trying to force the universe to be deterministic... We’re building institutions around non-deterministic thinkers so they can be creative inside boundaries and still ship on time.\n\nThere are four patterns I want you to internalize... Planner to Executor to Critic as your spine... Debate when you need structured disagreement... Router and Specialist to match tasks to the best mind... And Constitutional Guardrails to keep the whole society safe and compliant... Think of these as separation of powers for agent systems: plan, act, judge, and govern.\n\nLet’s start with Planner to Executor to Critic... You begin with a goal... A planner decomposes it into a graph of tasks... That graph gives you parallelism and clarity... Executors then perform those tasks with tools under strict contracts... And a critic validates the outputs against schemas, business rules, and policy... If the critic accepts, we assemble and deliver... If not, we repair and loop... The loop is the point: the critic gives the system the right to say no and demand better.\n\nIn practice, I recommend three things for PEC... First, type everything... Executors return structured JSON that you validate deterministically before you even read it... Second, cap the loop with iteration limits and circuit breakers... Non-deterministic processes need fences... Third, differentiate validators... Syntax and schema checks are fast and deterministic; semantic and policy checks can use LLMs with rubrics, but always after the basics pass.\n\nNow, when do we use Debate?... When the space of possible answers is open-ended, or when the cost of being wrong is high enough to justify multiple perspectives... Two or more agents propose solutions... They can cross-examine each other to expose weaknesses, and then a judge scores the proposals using a rubric that encodes what you care about: groundedness in evidence, completeness, and risk or cost... The outcome is not just a winner, but a rationale you can trace... The trick is to keep the rubric concrete and the number of rounds bounded... Remember: debate turns stochasticity into a controlled ensemble, but it’s not free... Use it where it increases expected quality per dollar and time.\n\nNext, Router and Specialist... Most workloads are heterogeneous... You don’t need a heavyweight reasoner to route a ticket... Use a small, fast model to classify intent and estimate risk... Then dispatch to a specialist with a prompt and tool belt tailored to that domain... Billing, tech, legal, or a generalist fallback... Add quotas and concurrency limits per lane to keep your SLOs predictable... The router lets you run cheaper and faster and improves quality because specialists are simpler to constrain... Pair every specialist with the same critic or with a domain-specific critic... That way, you keep policy consistent even as the execution logic diverges.\n\nFinally, Constitutional Guardrails... Guardrails are rules as code that sit alongside critics... Critics judge quality and fitness for purpose... Guardrails enforce non-negotiables: what must never happen, and how to repair or refuse when it does... For example, redact PII before anything leaves the sandbox... Or block a funds transfer above a threshold unless an approval ticket is attached... Express these as machine-enforceable predicates that run before or in parallel with critics... A good mental model is a judiciary with emergency powers: critics deliberate; guardrails interdict... Keep the rules auditable, versioned, and testable with unit cases just like you test prompts.\n\nLet’s put it all together... Start with PEC as your backbone... Every flow should have a plan, an execution phase, and a critic-driven convergence loop... Layer in a router up front to triage and allocate to specialists so you can control cost and latency... Where uncertainty is high—new domains, creative synthesis, adversarial settings—insert debate within the execution step, and have a judge pick the best argument under a clear rubric... And wrap the whole system in constitutional guardrails that check outputs and tool actions for violations, repairing or refusing before anything escapes the sandbox.\n\nA quick word on metrics... If you’re operating these patterns, watch first-pass yield—how often you accept on the first try... Track rounds to completion to see if your critic is too strict or your planner is under-specifying... Measure cost and latency per successful task, not per attempt, and keep an eye on violation rates from your guardrails... These numbers tell you where to tune: plan better, specialize more, debate less or more, tighten or loosen rules.\n\nAnd some pitfalls to avoid... Don’t ship without a critic; you’ll end up chasing ghosts in production... Don’t let debate become unbounded; two rounds with a tight rubric gets you most of the value... Don’t route without budgets; backpressure and quotas keep the system healthy... And don’t confuse guardrails with critics; one is about safety and compliance, the other about quality and correctness... You need both.\n\nPhilosophically, what we’re doing is institutional design... We create norms through contracts and schemas... We create a judiciary through critics and judges... We establish a constitution through guardrails... And we allow a marketplace of ideas through planning and debate... We aren’t eliminating uncertainty—we’re circumscribing it... And within those boundaries, emergence becomes a feature you can rely on to make your customers’ dreams come true.\n\nIn the next segment, we’ll look at how to instrument these patterns with traces, budgets, and evaluation suites so you can operate them with confidence... For now, remember: Planner to Executor to Critic is your spine... Router and Specialist saves money and time... Debate buys you quality when it matters... And Constitutional Guardrails keep you safe... Build the institution first; then let the agents negotiate meaning inside it.",
      "duration": 7,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s5.tsx",
      "audioPath": "/audio/slide-s5.mp3"
    },
    {
      "id": "s6",
      "content": {
        "type": "markdown",
        "title": "Live build from first principles: agent contracts, tool adapters, planner skeleton, orchestrator loop",
        "markdown": "- We don't eliminate uncertainty; we bound it with contracts\n- From goal -> plan (DAG) -> execute with tools -> verify -> converge\n- We'll live-build 4 primitives: agent contract, tool adapter, planner skeleton, orchestrator loop\n- Philosophy: choose norms and let emergence happen inside guardrails\n---\n\n## Agent contract (small, strict)\n- Inputs: context, allowed tools, budget/timeouts, memory slice\n- Outputs: structured_result, tool_calls, next_actions, metrics\n- Determinism via JSON schema + validator\n```json\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"title\": \"AgentOutput\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"structured_result\": {\"type\": \"object\"},\n    \"tool_calls\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"type\": \"object\",\n        \"properties\": {\n          \"name\": {\"type\": \"string\"},\n          \"args\": {\"type\": \"object\"},\n          \"status\": {\"type\": \"string\", \"enum\": [\"success\", \"error\"]}\n        },\n        \"required\": [\"name\", \"args\", \"status\"],\n        \"additionalProperties\": false\n      }\n    },\n    \"next_actions\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    \"metrics\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"tokens\": {\"type\": \"integer\"},\n        \"latency_ms\": {\"type\": \"number\"},\n        \"confidence\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}\n      },\n      \"required\": [\"tokens\", \"latency_ms\"]\n    }\n  },\n  \"required\": [\"structured_result\", \"tool_calls\", \"metrics\"],\n  \"additionalProperties\": false\n}\n```\n```python\n# minimal, framework-agnostic agent runner\nfrom jsonschema import validate, ValidationError\n\ndef run_agent(input_ctx, model, schema, system_prompt):\n    prompt = system_prompt.format(**input_ctx)\n    raw = model.generate(prompt, temperature=0.2, response_format=\"json\")\n    try:\n        validate(instance=raw, schema=schema)\n    except ValidationError as e:\n        return {\"ok\": False, \"error\": f\"schema_error: {e.message}\", \"raw\": raw}\n    return {\"ok\": True, \"value\": raw}\n```\n---\n\n## Tool adapter (typed, sandboxed)\n- Name/version, JSON args, typed return\n- Side-effect policy: read-only vs write; idempotency_key\n- Retries with backoff, timeouts, quotas\n```python\nfrom pydantic import BaseModel, Field\nimport time\n\nclass SearchArgs(BaseModel):\n    query: str\n    top_k: int = Field(5, ge=1, le=10)\n\nclass SearchResult(BaseModel):\n    items: list[dict]  # {title, url, snippet}\n\nclass Tool:\n    def __init__(self, name, version, func, read_only=True):\n        self.name = name; self.version = version\n        self.func = func; self.read_only = read_only\n\n    def call(self, args: BaseModel, timeout_s=8, idempotency_key=None):\n        start = time.time()\n        # quota, rate-limit, and sandbox checks go here\n        out = self.func(args)\n        if time.time() - start > timeout_s:\n            raise TimeoutError(self.name)\n        return out\n\n# example adapter\ndef search_web(args: SearchArgs) -> SearchResult:\n    # call your API here; mock for demo\n    return SearchResult(items=[{\"title\": \"Doc\", \"url\": \"https://...\", \"snippet\": \"...\"}])\n\nSEARCH_TOOL = Tool(\"search_web\", \"1.0.0\", search_web, read_only=True)\n```\n---\n\n## Planner skeleton (decompose -> DAG)\n- Returns tasks and dependencies\n- Keep it explicit and auditable\n```python\ndef plan(goal: str, constraints: dict):\n    tasks = [\n        {\"id\": \"A\", \"name\": \"research\", \"budget\": 0.01},\n        {\"id\": \"B\", \"name\": \"outline\",  \"budget\": 0.005},\n        {\"id\": \"C\", \"name\": \"draft\",    \"budget\": 0.03},\n        {\"id\": \"D\", \"name\": \"review\",   \"budget\": 0.005},\n    ]\n    deps = [(\"A\", \"C\"), (\"B\", \"C\"), (\"C\", \"D\")]  # edges: from -> to\n    return {\"tasks\": tasks, \"deps\": deps}\n```\n---\n\n## Orchestrator loop (centralized)\n- Ready set = tasks with deps satisfied\n- Parallelize independent tasks\n- Validate, update DAG, handle failures\n```python\nfrom concurrent.futures import ThreadPoolExecutor\n\ndef orchestrate(goal, model):\n    st = {\"plan\": plan(goal, {}), \"artifacts\": {}, \"failures\": {}, \"done\": False}\n    with ThreadPoolExecutor(max_workers=3) as pool:\n        while not st[\"done\"]:\n            runnable = [t for t in st[\"plan\"][\"tasks\"] if ready(t, st)]\n            if not runnable: break\n            futures = [pool.submit(run_task, t, model, st) for t in runnable]\n            for f in futures: f.result()\n            st[\"done\"] = all_completed(st)\n    return assemble(st)\n\n# helper stubs: ready, run_task, all_completed, assemble\n```\n---\n\n## DAG: planner -> executors -> critic\n```mermaid\ngraph LR\n  A[Research] --> C[Draft]\n  B[Outline] --> C\n  C --> D[Review]\n  D -->|accept| E[Done]\n  D -.->|fix| C\n```\n---\n\n## Execution sequence (one task)\n```mermaid\nsequenceDiagram\n  participant U as User Goal\n  participant O as Orchestrator\n  participant P as Planner\n  participant X as Agent(Executor)\n  participant T as Tool(search_web)\n  participant C as Critic\n\n  U->>O: goal\n  O->>P: plan(goal)\n  P-->>O: tasks + DAG\n  O->>X: run(research, tools=[T])\n  X->>T: call(query)\n  T-->>X: results\n  X-->>O: JSON output\n  O->>C: validate(output)\n  C-->>O: accept/fix\n  O-->>U: artifact or iteration\n```\n---\n\n## Flow control with quality gates\n- Schema -> business rules -> policy -> accept/repair\n- Circuit breakers, retries, max-iterations\n```mermaid\nflowchart TD\n  S[Start] --> R[Ready tasks]\n  R --> E[Execute agent]\n  E --> V[Validate output]\n  V -->|pass| UDT[Update DAG]\n  V -->|fail| RP[Repair/Retry]\n  RP --> R\n  UDT --> D{All done?}\n  D -->|yes| F[Finish]\n  D -->|no| R\n```\n---\n\n## Minimal live test outline\n- Run planner -> view DAG\n- Execute research with `SEARCH_TOOL`\n- Validate via schema + critic\n- Update DAG and iterate"
      },
      "speakerNotes": "Overall intent: philosophical but hands-on. Show the four primitives, wire them, and visualize the DAG. Keep momentum, avoid deep framework rabbit holes.\n\n0:00–0:30\n- Set the frame: we bound uncertainty with contracts and governance.\n- Advance to Agent Contract slide.\n\n0:30–2:00 Agent Contract\n- Walk through the JSON schema: emphasize required fields and additionalProperties=false.\n- Stage: Switch to editor/IDE; show schema file.\n- Demo: Paste a small valid and invalid example; mention validator catching errors.\n- Timing: Spend ~60 seconds on code.\n- Prompt audience: “Notice how confidence is optional and bounded.”\n\n2:00–3:10 Tool Adapter\n- Explain typed args, read_only flag, and idempotency idea.\n- Stage: Show the Tool class and SearchArgs/SearchResult.\n- Technical reminder: Mention timeouts and quotas tie into ops.\n- Optional quick run: call `SEARCH_TOOL.call` with a sample query.\n\n3:10–4:00 Planner Skeleton\n- Philosophy: plans are promises to the future; keep them explicit.\n- Show plan() code returning tasks and deps.\n- Ask: “Why DAG? So we can parallelize and reason about convergence.”\n\n4:00–5:30 Orchestrator Loop\n- Stage: Switch to terminal; show orchestrate() snippet.\n- Emphasize ready-set calculation, parallel execution, and validation before updating DAG.\n- Technical reminder: mention max_workers concurrency and where critics plug in.\n\n5:30–6:30 Diagrams\n- Show DAG mermaid; narrate A/B to C to D with fix loop.\n- Show sequence diagram; narrate the path from goal to critic.\n- Pause 5 seconds to let them parse.\n\n6:30–7:30 Flow control and quality gates\n- Point to flowchart: schema -> rules -> policy -> accept/repair.\n- Remind: circuit breakers, max iterations, and watchdogs live here.\n\n7:30–8:00 Mini-run-through\n- Stage: “Run” the happy path verbally: plan -> research -> validate -> draft -> review.\n- Close with philosophy: we design institutions, not just agents.\n\nInteractions\n- Ask: “Who’s already using schemas at tool boundaries?”\n- Check for quick questions if time permits.\n\nTechnical reminders\n- Keep temperature low in the runner.\n- Call out additionalProperties=false as a guardrail.\n- Note where budgets/timeouts would be enforced.\n- If a live run fails, classify and show how the repair path would handle it.\n\nContingency\n- If the search tool demo stalls, mock the result and proceed to validation.\n- If Mermaid doesn’t render, briefly describe the edges verbally.",
      "narration": "Let’s build this from first principles... Building with non-deterministic learners isn’t about forcing determinism... It’s about choosing norms, then letting emergence happen inside guardrails... In practice, that means four primitives: agent contracts, tool adapters, a planner that returns a DAG, and a simple orchestrator loop to drive execution with quality gates.\n\nWe start with the agent contract... Think of the contract as the constitution for one reasoning-and-acting component... Inputs are the task context, allowed tools, budgets, and a slice of memory... Outputs must be structured and small: a structured_result object, the tool_calls that actually happened, optional next_actions, and metrics like tokens and latency... The key is that the contract is strict and machine-validated... If the model emits anything outside the schema, we catch it immediately and either repair or refuse... This is how we wrap non-determinism: by insisting on predictable shapes and by logging every tool call.\n\nIn the runner, we render a deterministic prompt and ask the model for JSON... Then we validate it against the schema... If it fails, we classify that as a schema error and take the repair path... When it passes, we have a clean artifact we can route and reason about... Notice how confidence can be bounded between zero and one, but it’s optional... We prefer hard validators to vibes.\n\nNext, tools... In multi-agent systems most of the actual power sits in tools, not in prompts... So we build a tool adapter with a name, a version, typed arguments, and a typed return... We declare whether a tool is read-only or write-capable... We add timeouts, quotas, and idempotency keys so repeated calls don’t create surprising side effects... If a tool can change the world, we make that explicit and gate it with policy.\n\nIn our example, we define a search_web tool... The adapter class validates inputs, enforces time limits, and returns a typed result... This is where least-privilege lives: each agent gets a scoped list of tools, and each tool is sandboxed... By the time a model tries to act, the rails are already in place.\n\nNow we need a plan... Plans are promises to the future, expressed as a DAG... Our planner takes a goal and returns a set of tasks and dependencies... Here, we keep it simple: research and outline feed into draft, which feeds into review... The point of returning an explicit DAG is twofold... First, we can parallelize independent tasks, which reduces latency... Second, we gain observability and control... Every edge is a reason, and every node is a checkpoint where we can validate, repair, or stop.\n\nWith a plan in hand, we drive it with a minimal orchestrator loop... The orchestrator computes the ready set—those tasks whose dependencies are satisfied—runs them, validates outputs, and updates the DAG... We parallelize independent tasks but keep concurrency bounded... We record every artifact and every failure with reasons... And critically, we do not advance the DAG on invalid outputs... Instead we take the repair path: retry, request more evidence, downgrade the task’s scope, or escalate to a human... This loop is mundane by design... Orchestration is governance, not magic.\n\nLet’s visualize it... In the DAG, research and outline flow into draft... Draft flows into review... Review can accept and finish, or send us back to fix the draft... That feedback edge is where quality becomes institutional: we don’t hope for correctness; we route for it... In the sequence view, the user provides a goal... The orchestrator asks the planner for a DAG... It assigns a research task to an agent with the search tool... The agent calls the tool, returns a JSON output, and we pass that through a critic... The critic can accept or send us to repair... Finally, the orchestrator either moves forward or loops... What matters is the shape: plan, act, verify, and only then commit.\n\nUnder the hood, the flow control mirrors this... Start with ready tasks, execute with the assigned agent and tools, validate through schema and business rules and policy, and either update the DAG or repair and retry... We add circuit breakers for flapping steps, a maximum number of iterations to prevent infinite loops, and watchdogs for deadlocks... None of these require a fancy framework... They require discipline and clear boundaries.\n\nLet me narrate a tiny run... We receive the goal: write a short report with citations... The planner returns four tasks: research, outline, draft, review, with edges from research and outline into draft, and from draft into review... The orchestrator sees that research and outline are ready... It runs the research agent with the search tool... The agent calls the tool, returns a structured_result with a few candidate sources and a list of tool_calls... We validate the JSON shape, then we check business rules: at least one source with a URL, no PII in snippets... That passes, so we update the DAG.\n\nIn parallel, the outline agent produced a bullet structure... Now draft becomes ready... We run the drafting agent with the citation extraction tool... It returns a draft that references citation IDs... The critic checks that every claim maps to an evidence citation... If a claim is missing a citation, the critic flags it, and the orchestrator routes a repair instruction back to the draft agent, perhaps asking for more evidence... When it passes, we move to review... The reviewer checks tone, policy, and length... On accept, we’re done... At each step, the contract made outputs predictable; the tool adapters made actions safe; the DAG gave us control and parallelism; and the orchestrator enforced quality gates before advancing.\n\nPhilosophically, notice what we did not do... We didn’t try to make the model deterministic... We made the system legible... We chose institutions: a constitution in the form of contracts, a marketplace of tools with permissions, a judiciary of validators and critics, and a legislature of policies and budgets... Within those institutions, we let the agents negotiate meaning and we accept emergence, but only within boundaries we can audit.\n\nTo close, here’s the mental checklist... Define your contracts first... Build minimal tool adapters with strict types and side-effect policies... Write a planner that emits a DAG you can inspect... Implement a small orchestrator loop that runs ready tasks, validates outputs, updates state, and retries with guardrails... Add tracing and budgets from day one... If you do just these four things, you will ship reliably, even as your models evolve... That’s how we turn uncertainty into progress, and how complex multi-agent systems start making your customers’ dreams come true.",
      "duration": 8,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s6.tsx",
      "audioPath": "/audio/slide-s6.mp3"
    },
    {
      "id": "s7",
      "content": {
        "type": "markdown",
        "title": "Taming non‑determinism: schemas, sampling discipline, validation layers, repair loops",
        "markdown": "**Why non-determinism matters (and why we don’t fight it, we frame it)**\n- Uncertainty is a feature of learning systems; our job is to bound it with contracts.\n- Govern with schemas, sampling discipline, validation layers, and repair loops.\n- Separate creative phases from precise phases to reduce chaos where it hurts.\n- Trace everything: runs, tools, costs, and decisions become your debugger.\n```mermaid\nflowchart LR\n  U[User Goal] --> O[Orchestrator]\n  O --> A[\"Agent(s)\"]\n  A -->|Outputs| V[Validators]\n  V -->|Pass| R[Result]\n  V -->|Fail| F[Repair Loop]\n  F --> A\n```\n**Schemas and contracts (make uncertainty legible)**\n- Use strict JSON Schema or function-calling for outputs.\n- Deny unknown fields; enforce enums and patterns.\n- Version schemas; pin model versions per schema.\n- Fail fast on schema errors; never silently coerce.\n```ts\n// TypeScript: strict agent output schema using AJV\nimport Ajv from \"ajv\";\nconst ajv = new Ajv({allErrors: true, strict: true});\n\nconst outputSchema = {\n  $id: \"agent.output.v1\",\n  type: \"object\",\n  additionalProperties: false,\n  required: [\"status\", \"structured_result\", \"tool_calls\", \"metrics\"],\n  properties: {\n    status: { enum: [\"ok\", \"needs_more_info\", \"fail\"] },\n    structured_result: {\n      type: \"object\",\n      additionalProperties: false,\n      required: [\"summary\", \"citations\"],\n      properties: {\n        summary: { type: \"string\", minLength: 1 },\n        citations: {\n          type: \"array\",\n          items: { type: \"string\", pattern: \"^CIT-\\\\d+$\" },\n          minItems: 1\n        }\n      }\n    },\n    tool_calls: {\n      type: \"array\",\n      items: {\n        type: \"object\",\n        additionalProperties: false,\n        required: [\"name\", \"args\", \"status\"],\n        properties: {\n          name: { type: \"string\" },\n          args: { type: \"object\" },\n          status: { enum: [\"success\", \"error\"] }\n        }\n      }\n    },\n    metrics: {\n      type: \"object\",\n      additionalProperties: false,\n      required: [\"tokens\", \"latency_ms\"],\n      properties: {\n        tokens: { type: \"integer\", minimum: 0 },\n        latency_ms: { type: \"integer\", minimum: 0 }\n      }\n    }\n  }\n};\n\nconst validate = ajv.compile(outputSchema);\nexport function check(output: unknown) {\n  if (!validate(output)) throw new Error(ajv.errorsText(validate.errors));\n  return output; // typed and trusted\n}\n```\n**Sampling discipline (separate creativity from precision)**\n- Low temperature for schema-bound steps; higher for ideation.\n- Use best-of-N with a judge for brittle steps; keep N small.\n- Fix seeds where supported; otherwise log logprobs for audit.\n- Stable prompts: canonical templates; explicit role and constraints.\n```python\n# Python-like pseudocode\nPHASES = {\n  \"plan\":    dict(temp=0.7, top_p=0.95, n=1),     # creative\n  \"execute\": dict(temp=0.2, top_p=0.9,  n=1),     # precise\n  \"critique\":dict(temp=0.0, top_p=1.0,  n=3)      # judge best-of-3\n}\n\ndef run_step(phase, prompt, schema=None):\n  cfg = PHASES[phase]\n  outs = [model.generate(prompt,\n                         temperature=cfg[\"temp\"],\n                         top_p=cfg[\"top_p\"],\n                         response_format=schema) for _ in range(cfg[\"n\"])]\n  if phase == \"critique\":\n    return judge_select(outs)  # LLM-as-judge with rubric + deterministic checks\n  return outs[0]\n```\n**Validation layers (syntax → semantics → policy)**\n- Syntax: JSON schema, type checks, allowed enums.\n- Semantics: business rules, unit checks, referential integrity.\n- Policy: compliance, safety, PII, tool permissions.\n- Tool execution: dry-run, idempotency, side-effect gating.\n```mermaid\nflowchart TD\n  O[Agent Output] --> S[Schema Validator]\n  S -->|ok| B[Business Rules]\n  S -->|fail| RP[Repair]\n  B -->|ok| P[Policy & Safety]\n  B -->|fail| RP\n  P -->|ok| AC[Accept]\n  P -->|fail| RJ[Reject or Human]\n  RP --> O\n```\n**Repair loops (tight, bounded, and typed)**\n- Always include a reason for failure and a targeted fix hint.\n- Limit iterations; add circuit breakers and fallbacks.\n- Prefer surgical edits over full re-generation.\n```python\nMAX_ITERS = 3\n\ndef repair_loop(task, attempt):\n  for i in range(MAX_ITERS):\n    try:\n      out = run_agent(task)\n      validate_schema(out)\n      check_business_rules(out)\n      check_policy(out)\n      return out  # success\n    except ValidationError as e:\n      attempt = attempt + 1\n      hint = make_repair_hint(e)  # cite exact path: e.g., $.citations[0]\n      task.prompt = inject_hint(task.prompt, hint)\n      if attempt >= MAX_ITERS:\n        return hard_fail(e)\n  return hard_fail(\"exceeded repairs\")\n```\n```mermaid\nsequenceDiagram\n  participant A as Agent\n  participant V as Validators\n  participant R as Repairer\n  A->>V: Produce JSON output\n  V-->>A: Schema error at $.citations[0]\n  V->>R: Emit repair hint + diff\n  R-->>A: Apply targeted fix instruction\n  A->>V: Regenerate minimally\n  V-->>A: Pass\n```\n**Quality Control Diagram (integrated in orchestration)**\n- Planner→Executor→Validator→Critic→Repair→Trace.\n- Every edge logged with metrics and provenance.\n- Accept only when all gates pass or escalate.\n```mermaid\nsequenceDiagram\n  autonumber\n  participant U as User\n  participant O as Orchestrator\n  participant P as Planner\n  participant X as Executor\n  participant T as Tool(s)\n  participant V as Validators\n  participant C as Critic/Judge\n  participant B as Blackboard/Trace\n\n  U->>O: Goal + constraints\n  O->>P: Decompose into tasks (DAG)\n  P-->>O: Tasks + deps\n  loop ready tasks\n    O->>X: Assign task + schema + budget\n    X->>T: Tool calls (sandboxed)\n    T-->>X: Typed results\n    X-->>V: Structured output (JSON)\n    V-->>C: Validated artifact or errors\n    alt pass\n      C-->>O: Accept\n      O->>B: Log artifact + metrics\n    else fail\n      C-->>O: Reasons + repair hints\n      O->>X: Repair with targeted instructions\n    end\n  end\n  O-->>U: Final result + trace id\n```\n**Metrics, budgets, and drift checks (close the loop)**\n- Track first-pass yield, rounds-to-accept, p95 latency, cost.\n- Budget caps per agent; circuit breakers on repeated failures.\n- Regression tests on schema/rules/model updates.\n```mermaid\npie title Validation failure taxonomy (last 7 days)\n  \"Schema\" : 35\n  \"Business rules\" : 25\n  \"Policy\" : 15\n  \"Tool errors\" : 20\n  \"Other\" : 5\n```"
      },
      "speakerNotes": "Overall pacing: 7 minutes total. Keep energy philosophical yet practical.\n\n0:00–0:30 — Set the frame\n- Say: We don’t eliminate uncertainty; we circumscribe it with contracts and feedback.\n- Pause for 2 seconds to let the idea settle.\n\n0:30–1:20 — Why non-determinism matters\n- Point to the first diagram; emphasize orchestration around validators and repair.\n- Ask: Who here has been bitten by a silent schema drift? (hands)\n\n1:20–2:20 — Schemas and contracts\n- Spend ~60 seconds walking through the TypeScript schema. Highlight additionalProperties: false.\n- Technical reminder: If showing code, zoom into enum and pattern fields.\n- Mention versioning: schema IDs and model pinning.\n\n2:20–3:10 — Sampling discipline\n- Contrast creative vs precise phases; point at PHASES dict.\n- Note best-of-N for critique only; keep N small.\n- Stage direction: Emphasize canonical prompts and logprobs.\n\n3:10–4:05 — Validation layers\n- Use the flowchart to explain syntax → semantics → policy order.\n- Give one concrete business rule example verbally (e.g., totals must sum).\n- Pause 2 seconds before moving on.\n\n4:05–5:05 — Repair loops\n- Show the sequence diagram; stress targeted hints with JSON paths.\n- Timing cue: 60 seconds here; mention iteration limits and circuit breakers.\n- Interaction prompt: Quick show of hands—who retries without changing the prompt?\n\n5:05–6:05 — Quality Control Diagram (integrated)\n- Walk the sequence from User to Orchestrator to Critic and back.\n- Technical reminder: Call out logging to Blackboard/Trace on every pass/fail.\n- Note: This is the backbone you can implement in any framework.\n\n6:05–6:45 — Metrics, budgets, drift\n- Show pie chart; talk about failure taxonomy informing fixes.\n- Mention first-pass yield and rounds-to-accept as north-star metrics.\n- Reminder: tie budgets and circuit breakers to these metrics.\n\n6:45–7:00 — Close\n- Reiterate philosophy: govern uncertainty; don’t deny it.\n- Invite questions for Q&A later; transition to next segment.\n\nTech prep reminders\n- Switch to code view for TypeScript and Python snippets.\n- Ensure Mermaid diagrams render; have static PNGs as fallback.\n- Keep terminal hidden for this segment; no live API calls.\n- If time is tight, skip detailed AJV errors and go straight to QC sequence.",
      "narration": "Let’s talk about taming non-determinism... Not by pretending it doesn’t exist, but by wrapping it in contracts, feedback, and governance so we can ship reliably... In other words, we don’t fight uncertainty; we frame it... In multi‑agent systems, that frame looks like schemas, sampling discipline, validation layers, and repair loops.\n\nStart with the mental model on the slide: an orchestrator sends work to agents, agents produce structured outputs, those outputs flow through validators, and when something fails, we don’t panic, we repair... This is the essential rhythm of reliable orchestration under uncertainty.\n\nContracts come first... If an agent is a process that negotiates meaning, the schema is the treaty that defines the borders of that meaning... We use strict JSON Schema or function calling... Deny unknown fields... Enforce enums and patterns for IDs and statuses... And version everything: schema IDs, model versions, even tool versions... Fail fast on schema violations... The worst bugs are the quiet ones where we auto‑coerce bad outputs and only discover it downstream... You can see in the code how additionalProperties is false, and how we validate and throw on any mismatch... That’s not cruelty; it’s kindness to your future self.\n\nOnce the contract is in place, we practice sampling discipline... Think in phases... Planning is creative—let temperature breathe a little... Execution is precise—lower the temperature and narrow top‑p... Critique often benefits from best‑of‑N, but keep N small and add a judge with a rubric... If your model supports it, fix seeds... If it doesn’t, log logprobs and full traces so you can at least replay deterministically around the model... And use canonical prompts... A canonical prompt is a stable template with explicit roles, constraints, and examples... Stability in prompts is the cheapest determinizer you have.\n\nWith outputs in hand, we pass through validation layers in a strict order: syntax, semantics, policy... Syntax is your schema—types, enums, patterns... Semantics are your business rules and unit checks: do totals add up?... Does every claim reference a citation in memory?... Policy is safety and compliance: no PII leakage, tool permissions honored, and refusals when necessary... Tool execution deserves its own gate: prefer dry‑runs and previews for side effects, and make every tool call idempotent or at least detectable if replayed.\n\nNow, what happens when validation fails?... We repair—deliberately and within bounds... A good repair loop does three things... First, it returns a precise reason tied to the exact JSON path or rule that failed... Second, it provides a targeted hint that asks for a surgical fix instead of a full re‑generation... Third, it enforces limits: a small number of iterations, circuit breakers for repeated failure classes, and a fallback or escalation path... In the diagram, notice how validators emit a specific error like a schema mismatch at a particular path, the repairer injects a focused instruction, and the agent regenerates minimally... This preserves context, reduces cost, and avoids spinning in loops.\n\nLet’s zoom out to the full quality control diagram... A user submits a goal with constraints... The orchestrator asks a planner to decompose that goal into a task graph... For each ready task, we assign an executor along with its schema, budget, and allowed tools... The executor may call tools, but those calls are sandboxed and typed... The structured output goes through validators... A critic or judge then assesses the artifact, combining deterministic checks with rubric‑based evaluation... On pass, we log the artifact, metrics, and provenance to a trace or blackboard... On fail, we propagate reasons and repair hints back to the executor... The orchestrator keeps looping until the task graph is complete, budget limits are reached, or a termination condition is met... Finally, we return a result along with a trace ID so we can replay and audit.\n\nAll of this only works if we close the loop with metrics and budgets... Track first‑pass yield: the percentage of tasks accepted without repair... Track rounds to accept: how many iterations does it take to get to done... Watch p50 and p95 latency, and always associate cost with each step... Set budget caps per agent and per run, and trip circuit breakers when a failure class repeats... This is how you prevent one stubborn task from melting your wallet... Use regression suites with golden tasks whenever you change a schema, a rule, or a model... If quality drops in canary traffic, roll back immediately.\n\nThere’s a philosophical through line here... We don’t build single genius agents... We build societies of processes with a constitution... The schema is our constitution... The validators are our judiciary... The orchestrator and policies are our institutions... The marketplace of agents and tools is our economy... Emergence still happens, but within boundaries we understand, measure, and govern.\n\nSo if you remember one thing from this section, let it be this: embrace non‑determinism where creativity lives, and constrain it where correctness matters... Use strict schemas to make uncertainty legible... Use sampling discipline to separate art from accounting... Layer validators to convert guesses into guarantees... And when things go wrong—and they will—repair quickly, locally, and with empathy for both the model and the operator... That’s how we ship complex multi‑agent systems that make our customers’ dreams come true, reliably.\n\nIn the next segment, we’ll apply this pattern to a live use case and show how these gates and loops look in traces you can build today.",
      "duration": 7,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s7.tsx",
      "audioPath": "/audio/slide-s7.mp3"
    },
    {
      "id": "s8",
      "content": {
        "type": "markdown",
        "title": "Memory, governance, and safety by construction: state, PII, sandboxing, prompt‑injection defenses",
        "markdown": "- We design agent societies with memory and law. Safety emerges from boundaries, not from wishes.\n- Govern state explicitly: what can be remembered, for how long, and by whom.\n- Make PII flows visible and enforceable at write-time, not after an incident.\n- Sandboxing and prompt-injection defenses are architectural patterns, not bolt-on filters.\n---\n\n### State and Memory Boundaries (safety by construction)\n- Treat state as a first-class resource: run-state, blackboard, long-term memory.\n- Apply TTLs, summarization with provenance, and access policies per agent.\n- Separate system, user, and retrieved context; sign what must not change.\n```mermaid\nflowchart LR\n  U[User Goal] --> O[Orchestrator]\n  O --> A1[Agent A]\n  O --> A2[Agent B]\n  A1 & A2 --> BB[(Blackboard)]\n  BB --> SUM[Summarizer]\n  SUM --> BB\n  BB --> PII{PII Gate}\n  PII -->|redact| BB\n  C[Critic/Policy] --> BB\n  O --> Trace[(Trace & Budgets)]\n```\n```json\n// Example: memory slice contract surfaced to an agent\n{\n  \"memory\": {\n    \"run_scratch\": {\"ttl_s\": 3600},\n    \"blackboard_view\": [\"artifact:task-123\", \"artifact:policy-summary\"],\n    \"long_term_keys\": [\"case:similar-requests\"],\n    \"provenance_required\": true\n  }\n}\n```\n---\n\n### PII Governance: lifecycle and redaction\n- Classify and redact at ingestion; store only what policy allows.\n- Tag artifacts with sensitivity, retention, and lineage.\n- Audit every read/write; default deny for cross-domain access.\n```mermaid\nsequenceDiagram\n  participant U as User/Input\n  participant I as Ingest\n  participant C as Classifier\n  participant R as Redactor\n  participant E as Evidence Store\n  participant RM as Retention Manager\n  U->>I: submit data\n  I->>C: detect PII classes\n  C-->>I: labels (email, phone, secret)\n  I->>R: redact/transform\n  R-->>E: write redacted + hashes + provenance\n  RM-->>E: enforce TTL/holds\n```\n```python\n# Python: write-time PII scrubber with provenance\nPII_PATTERNS = {\n    \"email\": r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\",\n    \"phone\": r\"\\\\+?\\\\d[\\\\d\\\\-\\\\s]{7,}\\\\d\",\n    \"ssn\": r\"\\\\b\\\\d{3}-\\\\d{2}-\\\\d{4}\\\\b\"\n}\n\ndef scrub(record, policy, source):\n    labels = {k: bool(re.search(v, record)) for k, v in PII_PATTERNS.items()}\n    redacted = record\n    for label, pat in PII_PATTERNS.items():\n        if labels[label] and policy[\"pii\"][label][\"redact\"]:\n            redacted = re.sub(pat, f\"{{{{{label}_redacted}}}}\", redacted)\n    return {\n        \"text\": redacted,\n        \"provenance\": {\"source\": source, \"hash\": sha256(record)},\n        \"labels\": labels,\n        \"retention\": policy[\"retention\"],\n    }\n```\n---\n\n### Sandboxing and Least Privilege for Tools\n- Per-run credentials; scope networks, files, and side effects.\n- Deny-by-default egress; allowlist domains and methods.\n- Time, memory, and rate limits per tool call.\n```mermaid\ngraph LR\n  A[Agent] --> TP[Tool Proxy]\n  TP --> PE[Policy Engine]\n  PE --> SB[Sandbox]\n  SB --> EXT[(External APIs/DBs)]\n  Vault[(Secrets Vault)] --> TP\n  Logs[(Audit/Trace)] --> PE\n```\n```yaml\n# Tool sandbox policy (YAML)\nversion: 1\ntools:\n  search_web:\n    network:\n      egress_allow: [\"https://api.duckduckgo.com\", \"https://example.com\"]\n    filesystem: {read: [], write: []}\n    cpu_ms: 200, mem_mb: 128, timeout_s: 5, rate_per_min: 30\n  update_customer:\n    side_effects: write\n    approvals: [\"risk_owner\"]  # requires explicit approval token\n    network: {egress_allow: [\"https://internal.api\"]}\n    scopes: [\"customer:read\", \"customer:write:masked\"]\n```\n---\n\n### Prompt-Injection Defenses: layered and explicit\n- Segment system, user, and retrieved text; never merge blindly.\n- Require justification tied to signed context IDs before risky actions.\n- Validate intents; refuse attempts to alter policy or jailbreak.\n```mermaid\nsequenceDiagram\n  participant U as User\n  participant R as Retriever\n  participant S as Segmenter\n  participant L as LLM Agent\n  participant P as Policy/Guard\n  participant G as Tool Gate\n  U->>S: user_msg\n  R-->>S: docs_with_signatures\n  S-->>L: {system, user, context[]}\n  L->>P: propose tool_call + justification\n  P-->>G: validate against rules & signatures\n  alt injection detected\n    G-->>L: reject + ask for grounded plan\n  else allowed\n    G-->>Tool: execute\n  end\n```\n```python\n# Tool gating with justification & signature checks\n@dataclass\nclass ToolProposal:\n    name: str\n    args: dict\n    justification: str\n    cited_context_ids: list[str]\n\ndef guard(proposal: ToolProposal, context_index, policy) -> bool:\n    if proposal.name in policy.denylist:\n        return False\n    if is_instruction_injection(proposal.justification):\n        return False\n    if policy.requires_context[proposal.name]:\n        if not proposal.cited_context_ids:\n            return False\n        if not all(context_index.is_signed(cid) for cid in proposal.cited_context_ids):\n            return False\n    return risk_score(proposal) <= policy.max_risk\n```\n---\n\n### Governance Diagram: control plane around non-determinism\n- Contracts + schemas limit behavior; critics and rules verify.\n- Budgets, traces, and audits make emergence observable and steerable.\n- Human-in-the-loop for high-risk branches.\n```mermaid\nflowchart TD\n  Plan[Planner/DAG] --> Exec[Executors]\n  Exec --> Valid[Schema + Policy Validators]\n  Valid --> Critic[Critic/Judge]\n  Critic -->|fix or accept| Exec\n  Valid -->|pass| Done[Resolution]\n  subgraph Controls\n    Bud[Budgets/Quotas]\n    Tr[Trace/Audit]\n    HR[Human Review]\n  end\n  Exec -.-> Bud\n  Exec -.-> Tr\n  Critic -.-> HR\n```\n```json\n// Structured output contract with tool-call ledger\n{\n  \"result\": {\"type\": \"object\", \"properties\": {\"answer\": {\"type\": \"string\"}}},\n  \"tool_calls\": [{\n    \"name\": \"update_customer\",\n    \"args\": {\"id\": \"123\", \"fields\": {\"email\": \"{email_redacted}\"}},\n    \"justification\": \"Requested by user; cites ctx:doc_77\",\n    \"cited_context_ids\": [\"doc_77\"],\n    \"risk\": \"medium\",\n    \"approval_token\": null,\n    \"status\": \"dry_run\"\n  }]\n}\n```"
      },
      "speakerNotes": "- Overall timing: 5 minutes. Keep a brisk, calm pace. Philosophical tone.\n\nSlide 1 (State and Memory Boundaries) — 50s\n- Open with the idea: we govern memory like city zoning. Pause 3s.\n- Point at diagram: user → orchestrator → agents → blackboard; emphasize PII gate and summarizer loop.\n- Technical reminder: briefly mention provenance pointers and TTLs.\n\nSlide 2 (PII Governance) — 60s\n- Ask: “Where does PII get removed in your current stack?” Wait 2s for reflection.\n- Walk through sequenceDiagram: ingest → classify → redact → store → retention.\n- Call out code: redaction at write-time with hash/provenance.\n- Reminder: stress default-deny cross-domain access.\n\nSlide 3 (Sandboxing) — 55s\n- Transition: “Now, tools only act within sandboxes.”\n- Highlight policy engine and secrets vault in diagram.\n- Briefly read YAML: allowlist egress, approvals for side effects, rate limits.\n- Note: least privilege per run, not per service.\n\nSlide 4 (Prompt-Injection Defenses) — 70s\n- Prompt the audience: “What if the retrieved doc tells the model to ignore policy?”\n- Explain segmentation and signed context; justification before tools.\n- Point at guard() and the rejection path; emphasize layered checks.\n- Tip: mention refusing and asking for a grounded plan.\n\nSlide 5 (Governance Diagram) — 55s\n- Connect to thesis: critics, rules, budgets form the constitution.\n- Explain loop: execute → validate → critic → fix/accept → done.\n- Call out human-in-the-loop on high risk.\n- Close with the idea: observability makes emergence steerable.\n\nWrap-up — 20s\n- Reiterate: safety by construction; boundaries, not band-aids.\n- Invite questions for the Q&A segment later.\n\nTechnical reminders\n- Switch to slides with diagrams ready; no live demo here.\n- If asked, reference that policies live in code and config; show YAML quickly.\n- Keep examples short; avoid deep regex discussions.\n\nInteraction prompts\n- After PII slide: “Who redacts at write-time today?”\n- After injection slide: “Who segments system/user/context explicitly?”",
      "narration": "Let’s make a simple claim... We don’t make systems safe by wishing for determinism... We make them safe by drawing boundaries, writing a constitution, and enforcing it at the edges where information and actions flow... In multi‑agent orchestration, that constitution lives in memory, governance, and safety by construction.\n\nStart with state and memory... Think of memory like city zoning... Some zones are temporary, like a construction site—the run scratchpad... Some are shared commons—your blackboard... Some are long‑term archives... Each zone needs rules: who can write, who can read, how long artifacts live, and how they’re summarized... In the diagram, notice how the orchestrator routes agents through a blackboard, and a summarizer compresses artifacts while keeping provenance intact... A PII gate sits on the write path... That’s intentional: we don’t hope to catch sensitive data later; we enforce policy as data is born.\n\nNow, PII governance... Ask yourself: where, precisely, is redaction happening in your stack?... If the answer is “logs,” the incident has already occurred... The safer pattern is an ingestion pipeline: classify likely PII, transform it at write‑time, attach provenance and hashes, and then store the redacted form... Retention policy is not a spreadsheet—it's code... Our small Python snippet shows the idea: detect, replace with structured placeholders, record provenance, and push retention metadata alongside the artifact... Default‑deny any cross‑domain access, and make every read and write auditable.\n\nNext, sandboxing and least privilege... Agents should never hold keys to the kingdom... Instead, they use a tool proxy, governed by a policy engine, with secrets fetched per run... Egress is deny by default... You allow only the domains you intend... Side effects require approvals or tokens... Time, memory, and rate limits are not afterthoughts; they’re part of the contract... The YAML policy makes all this legible: what can call where, with what scope, for how long, and how often... When you parameterize power like this, you convert risk into configuration.\n\nLet’s confront prompt injection... Retrieval is a gift, but untrusted documents can carry instructions that try to rewrite your constitution... The defense is layered... First, segment the inputs: system policy, user goal, and retrieved context are separate lanes, not a blended smoothie... Second, sign the context you trust so that the model must cite specific, signed documents when justifying risky actions... Third, require justification before tools run, and validate that justification against policy... In the sequence here, the agent proposes a tool call with cited context IDs... The guard checks for injection cues, verifies signatures, and computes a risk score... If it fails, we reject and ask for a grounded plan... This isn’t just filtering; it’s due process.\n\nZooming back out, the governance control plane is how we steer emergence... Contracts and schemas bound what agents can say... Validators and critics measure outputs against business rules, safety policy, and semantics... Budgets, traces, and audits make every step observable and accountable... For high‑risk branches, we invite a human... The loop is simple: execute, validate, critique, repair, and either continue or conclude... With this loop, non‑determinism becomes manageable because it’s nested inside predictable scaffolding.\n\nIf there’s one mental model to take away, it’s this: safety lives in structure... When state is explicit, when PII is governed at birth, when tools are sandboxed with least privilege, and when prompt‑injection defenses are layered into the architecture, you stop firefighting and start engineering... You’re not suppressing emergence; you’re channeling it.\n\nAs you build your own agent societies, write your constitution first: define memory zones and retention, specify tool scopes and approvals, encode validators and critics, and commit to tracing everything... Then accept that the models will surprise you, and let your governance catch and shape those surprises... That’s how we ship reliably, and, more importantly, how we make our customers’ dreams come true with eyes wide open.",
      "duration": 5,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s8.tsx",
      "audioPath": "/audio/slide-s8.mp3"
    },
    {
      "id": "s9",
      "content": {
        "type": "markdown",
        "title": "Case study demo: research‑and‑write pipeline with citations and critics — End‑to‑End Trace",
        "markdown": "## What we’re building + success criteria\n- End-to-end research-and-write pipeline with citations and a critic gate\n- Contracts: strict JSON outputs; tools and budgets per agent\n- Deterministic quality gates: schema + citation validator\n- Full trace: spans, tool calls, tokens, latency, verdicts\n- Philosophy: we don’t remove uncertainty; we bound it with governance\n```json\n{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"title\": \"DraftWithCitations\",\n  \"type\": \"object\",\n  \"required\": [\"title\", \"sections\", \"citations\"],\n  \"properties\": {\n    \"title\": {\"type\": \"string\"},\n    \"sections\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n    \"claims\": {\n      \"type\": \"array\",\n      \"items\": {\"type\": \"object\", \"required\": [\"text\", \"citation_ids\"],\n        \"properties\": {\n          \"text\": {\"type\": \"string\"},\n          \"citation_ids\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n        }\n      }\n    },\n    \"citations\": {\n      \"type\": \"object\",\n      \"additionalProperties\": {\n        \"type\": \"object\",\n        \"required\": [\"url\", \"quote\"],\n        \"properties\": {\"url\": {\"type\": \"string\"}, \"quote\": {\"type\": \"string\"}}\n      }\n    }\n  }\n}\n```\n## System at a glance (conductor topology)\n```mermaid\nflowchart TD\n  U[User Brief] --> P[Planner]\n  P --> R[Research Agent]\n  R -->|search_web/get_pdf| T[(Evidence Store)]\n  R --> O[Outline Agent]\n  O --> D[Draft Agent]\n  D --> C[Critic/Judge]\n  C -->|accept| F[Final Report]\n  C -->|repair| R\n  subgraph Observability\n    X[\"Trace Collector]\\n(spans, costs, latencies)\"]\n  end\n  R -.-> X\n  D -.-> X\n  C -.-> X\n  T -. provenance .-> D\n```\n## Agent contracts + tools (framework-agnostic)\n- Small, strict interfaces; least-privilege tools\n- Separate creative vs precise steps via temperatures\n- Deterministic validators before/after model calls\n```python\nAgentInput = TypedDict(\"AgentInput\", {\n  \"task\": str, \"constraints\": dict, \"tools\": list, \"memory\": dict,\n  \"budget\": {\"tokens\": int, \"seconds\": int}\n})\n\nAgentOutput = TypedDict(\"AgentOutput\", {\n  \"result\": dict, \"tool_calls\": list, \"next_actions\": list,\n  \"metrics\": {\"tokens\": int, \"latency_ms\": int, \"confidence\": float}\n})\n\n@dataclass\nclass Tool:\n    name: str; version: str; schema: dict; fn: Callable\n\ndef search_web(query: str) -> list[dict]: ...  # deterministic args, typed return\n\ndef citation_check(draft: dict, evidence: dict) -> tuple[bool, str]:\n    for claim in draft.get(\"claims\", []):\n        for cid in claim.get(\"citation_ids\", []):\n            if cid not in evidence: return (False, f\"Missing citation: {cid}\")\n    return (True, \"ok\")\n```\n## Orchestrator loop + quality gates\n- Plan -> assign -> execute -> verify -> repair-or-accept\n- Parallelize independents; enforce budgets; circuit breakers\n- Log every span with inputs/outputs (redacted), tokens, costs\n```python\nDAG = plan(goal)\nstate = init_state(DAG)\nwhile ready := next_ready(DAG, state):\n    for task in ready:\n        agent = assign(task)\n        out = run_agent(agent, task, tools=allowed(task), temp=task.temp)\n        ok, why = validate_schema(out, task.schema)\n        if ok and task.name == \"draft\":\n            ok, why = citation_check(out[\"result\"], state.evidence)\n        record_span(task, out, verdict=ok, note=why)\n        if ok: commit(state, task, out)\n        else: repair(state, task, why)\n        if exceeded_limits(state): break\nfinal = assemble(state)\n```\n```mermaid\nflowchart LR\n  A[Agent Output] --> S[Schema Validator]\n  S -->|pass| R1[Rules/Business Checks]\n  S -->|fail| Fix1[Repair]\n  R1 -->|pass| J[Critic/Judge]\n  R1 -->|fail| Fix2[Repair]\n  J -->|accept| Done[Publish]\n  J -->|request fix| Fix3[Repair Loop]\n```\n## End-to-end trace (demo view)\n- Spans capture dependencies, tools, verdicts\n- Enables replay, regression, and cost/latency analysis\n```mermaid\nsequenceDiagram\n  participant U as User\n  participant P as Planner\n  participant R as Research\n  participant E as EvidenceStore\n  participant D as Draft\n  participant C as Critic\n  U->>P: goal\n  P->>R: task: research\n  R->>E: write evidence {id:url, quote}\n  P->>D: task: draft + memory slice\n  D->>E: read citations\n  D-->>C: draft.json (claims, citation_ids)\n  C-->>D: missing citation -> request repair\n  D->>E: add citation\n  D-->>C: revised draft\n  C-->>P: accept\n```\n```json\n[\n  {\"id\":\"1\",\"name\":\"plan\",\"dur_ms\":120,\"tokens\":420},\n  {\"id\":\"2\",\"parent\":\"1\",\"name\":\"research\",\"dur_ms\":9800,\n   \"tool_calls\":[{\"tool\":\"search_web\",\"q\":\"sodium-ion batteries 2024\"}],\n   \"artifacts\":3},\n  {\"id\":\"3\",\"parent\":\"1\",\"name\":\"draft\",\"dur_ms\":4200,\n   \"tokens\":860, \"citations_used\":[\"c1\",\"c2\"]},\n  {\"id\":\"4\",\"parent\":\"3\",\"name\":\"critic:citation_check\",\"dur_ms\":60,\n   \"verdict\":\"fail\",\"reason\":\"Missing citation: c3\"},\n  {\"id\":\"5\",\"parent\":\"3\",\"name\":\"draft:repair\",\"dur_ms\":1800,\n   \"tokens\":220, \"citations_used\":[\"c1\",\"c2\",\"c3\"]},\n  {\"id\":\"6\",\"parent\":\"3\",\"name\":\"critic:citation_check\",\"dur_ms\":55,\n   \"verdict\":\"accept\"}\n]\n```"
      },
      "speakerNotes": "Overall: 6 minutes. Keep pace brisk but clear. Philosophical tone, but show concrete steps.\n\n0:00–0:30\n- Open with context: non-determinism + governance.\n- Stage: Stand facing camera, then move to slides.\n- Line: “We don’t build agents; we build societies with constitutions.”\n\n0:30–1:20 — Slide: What we’re building + schema\n- Point at bullets: end-to-end, contracts, critic gate, trace.\n- Zoom into JSON Schema; emphasize required fields and citation_ids.\n- Prompt: Ask quick show of hands: “Who already enforces JSON schemas?”\n- Timing: Spend ~40s on schema, 10s on success criteria.\n\n1:20–2:20 — Slide: System at a glance (Mermaid flow)\n- Explain Planner → Research → Outline → Draft → Critic loop.\n- Call out Evidence Store and Trace Collector.\n- Note: This is centralized conductor; mention blackboard is an alternative.\n- Technical reminder: Mention least-privilege tools and provenance.\n\n2:20–3:20 — Slide: Agent contracts + tools\n- Switch to code focus. Highlight AgentInput/Output and Tool adapter.\n- Emphasize deterministic tool schemas; strict typing.\n- Walk through citation_check: simple, fast, decisive.\n- Timing: 60s; Pause 5s for the audience to read code.\n\n3:20–4:15 — Slide: Orchestrator + quality gates\n- Walk line-by-line through loop: plan → assign → execute → validate → repair.\n- Mention parallelism, budgets, circuit breakers.\n- Call out temperatures: creative vs precise.\n- Show Mermaid QC diagram; tie to validator layers.\n\n4:15–5:45 — Slide: End-to-end trace demo\n- Stage: Switch to terminal (or pre-recorded trace viewer). Command: `python run_demo.py --goal \"350-word market brief on sodium-ion batteries, 3 citations\"`.\n- If live: run and narrate spans as they appear. If latency, show pre-baked trace JSON.\n- Highlight: critic failure then repair; show tokens, durations.\n- Ask: “Notice how the verdict toggles from fail to accept — that’s governance.”\n- Technical reminder: Zoom in on parent/child span IDs.\n\n5:45–6:00 — Wrap\n- Tie back to thesis: contracts + critics + traces tame uncertainty.\n- Call to action: “Start with the critic and the constitution; frameworks second.”\n- Check for time; if extra 5s, invite questions to chat.\n\nContingencies\n- If search tool is slow: narrate with the pre-recorded JSON trace.\n- If code font is small: increase zoom to 150%.\n- Security note: remind that logs shown are synthetic; no PII.\n\nReminders\n- Speak clearly; keep energy; pause after showing the fail→repair moment.\n- Keep cursor movements deliberate; avoid jitter on diagrams.\n- Mention that all steps are framework-agnostic; swap models/tools freely under contracts.",
      "narration": "Let’s make this idea concrete with a six–minute case study: a research and write pipeline that insists on citations, and proves its work with an end‑to‑end trace.\n\nThe frame is simple but powerful... We don’t build single, perfect agents... We build small institutions... There’s a planner that decomposes the goal, a researcher that gathers evidence, a writer that synthesizes, and a critic that holds the line... Our job is not to eliminate uncertainty, but to wrap it with contracts, critics, and traceability so we can ship with confidence.\n\nHere’s our goal... Given a brief, produce a short report with claims, and every claim must map to at least one citation... We define success as three things: first, the output adheres to a strict JSON schema; second, the critic verifies that every claim has a real citation drawn from our evidence store; and third, we can replay the entire run from the trace, including tools, costs, and latencies... If those hold, we accept whatever creativity happens inside the bounds.\n\nTo make that enforceable, we start with the contract... The draft’s schema has a title, sections, and a list of claims... Each claim carries an array of citation IDs... Separately, there’s a citations object keyed by those IDs, each with a source URL and an exact quote... That structure does two things... It reduces ambiguity for the writer agent, and it gives the critic something deterministic to verify... When we evaluate, we don’t ask, “Does this sound right?” We ask, “Can we trace this claim to a specific citation in our evidence store?”\n\nNow let me walk the architecture... A centralized orchestrator accepts the user goal and asks the planner to decompose it... The planner creates a small DAG: research, outline, draft, review... The research agent has least‑privilege access to search and fetch tools and writes findings into the evidence store with stable IDs and provenance... The outline agent compresses the plan into a structure the draft agent can follow... The draft agent reads the memory slice and the evidence store, produces the JSON draft with citation IDs, and the critic runs two layers of checks: schema validation and a citation check that ensures every claim’s IDs exist in the evidence store... If the critic finds a missing or invalid citation, we don’t panic; we repair... We either ask the draft agent to ground the claim or ask the researcher to fetch more evidence... All of this is traced, so we can see the dance, not just the final note.\n\nLet’s glance at the code contracts... Agents accept a small, typed input with the task, constraints, the allowed tools, a memory slice, and a budget... They return a structured result, a list of tool calls, and basic metrics... Tools themselves expose a name, version, and a JSON schema for arguments... You can swap models or frameworks, but these contracts are the constitution... And the critic’s core check is intentionally boring: iterate over claims, make sure each citation ID exists in the evidence map... When the critic is simple, it’s reliable... When it’s reliable, you can trust the system to govern non‑determinism.\n\nThe orchestrator loop is equally straightforward... Plan, assign, execute, validate, and either accept or repair... We parallelize independent tasks, enforce budgets, and record a span for every step with inputs and outputs redacted as needed... For creative steps like drafting, we might run at a slightly higher temperature... For precise steps like schema adherence and citation checks, we run at low temperature or avoid sampling entirely... The control is in the wrapper, not just the model.\n\nLet’s run a live example... The brief is: write a three‑hundred‑fifty‑word market note on sodium‑ion batteries with at least three citations from the last eighteen months... The planner emits the DAG... The research agent calls the search tool a few times, writes three pieces of evidence into the store, each with a URL and a quote... The draft agent synthesizes the report using two of those citations... The critic kicks in and says, fail: missing citation c3 on one of the claims... Perfect... That’s the system doing its job... The orchestrator routes a repair: it asks the draft agent to ground the missing claim or the researcher to fetch one more source... We add citation c3, the draft updates its claim, and we run the critic again... This time it accepts.\n\nOn the trace view, notice the parent and child spans... There’s a planning span, then a research span with tool calls embedded, then a draft span, then two critic spans — one fail, one accept — and a short repair span in between... Each span carries duration, token counts, and success or failure... This is what lets you replay the run offline, compare models in a controlled way, and catch regressions before they hit production.\n\nA quick word on quality and cost... Because the contracts keep outputs structured, we can run deterministic validators first... That catches the cheap errors early... The critic acts as our judiciary: it applies the constitution, not vibes... And the trace gives us operations: p50 and p95 latencies, cost per step, tool failure rates... If we see repeated critic failures on missing citations, we can tune the draft prompt to require citation IDs per claim, or we can add a small retriever that suggests likely citations during drafting to reduce repair loops.\n\nTwo philosophical notes before we close... First, coordination choice matters more than your toolkit... Today we used a centralized conductor because it’s easy to reason about and demo... If your workloads grow, you can move to a blackboard or a marketplace, and the same contracts and critics will still hold... Second, don’t start by adding more agents... Start by writing the constitution: the schemas, the validator rules, the budgets, and the trace you expect to see... Once those are in place, emergence becomes a feature... Your researcher and writer can be creative inside safe boundaries.\n\nSo that’s our end‑to‑end case study... A small society of processes, a clear constitution, a critic that enforces it, and a trace that tells the whole story... With that pattern, you can make non‑deterministic systems do deterministic work: cite sources, respect policies, and make your customers feel like their dreams were designed on purpose... In the framework zone or not, the principle is the same: contracts, critics, and traces, all the way down.",
      "duration": 6,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s9.tsx",
      "audioPath": "/audio/slide-s9.mp3"
    },
    {
      "id": "s10",
      "content": {
        "type": "markdown",
        "title": "Operate like an engineer: evaluation, metrics, tracing, regression and canaries",
        "markdown": "- We don't eliminate uncertainty; we instrument it. Build feedback loops first, features second.\n- Make success observable: traces, metrics, evaluations, and gates.\n- Treat models like dependencies: version, test, canary, and roll back.\n\n### Ops diagram: from runs to decisions\n```mermaid\nflowchart TD\n    U[User Goal / Task] --> R[Run Orchestration]\n    R --> T[\"Trace Spans\\n(prompts, tools, costs, latencies)\"]\n    T --> O[Offline Replay / Evals]\n    O --> M[\"Metrics\\n(success, FPY, p50/p95, cost, validator fail)\"]\n    M --> G[\"Regression Gate\\n(compare vs goldens & SLOs)\"]\n    G -->|pass| C[Canary Rollout]\n    G -->|fail| RB[Rollback]\n    C --> FR[Full Release]\n    C -->|degrade| RB\n    RB --> R\n```\n### Minimal tracing (OpenTelemetry-style)\n```python\nfrom contextlib import contextmanager\nfrom time import monotonic\n\n@contextmanager\ndef span(name, attrs=None):\n    t0 = monotonic()\n    try:\n        yield\n        status = \"OK\"\n    except Exception as e:\n        status = f\"ERR:{type(e).__name__}\"\n        raise\n    finally:\n        t1 = monotonic()\n        log_span({\n            \"name\": name,\n            \"duration_ms\": int((t1 - t0)*1000),\n            **(attrs or {}),\n            \"status\": status,\n        })\n\nwith span(\"agent.run\", {\"model\": \"gpt-4o\", \"run_id\": run_id}):\n    out = model.generate(prompt, response_format=schema)\n    validate(out)\n```\n### Metrics you can act on\n- Task success rate, first-pass yield, rounds-to-done\n- Latency p50/p95, cost per task, tool success rate\n- Validator failure rate, groundedness/citation coverage\n- Drift: content change, embedding shift, prompt injection hits\n```mermaid\nclassDiagram\nclass Effectiveness {\n  +success_rate%\n  +first_pass_yield%\n  +rounds_to_done\n}\nclass Efficiency {\n  +latency_p50/p95_ms\n  +cost_per_task$\n}\nclass Reliability {\n  +tool_success_rate%\n  +validator_fail_rate%\n}\nclass Drift {\n  +embedding_shift\n  +source_content_change\n}\nEffectiveness <|-- Metrics\nEfficiency <|-- Metrics\nReliability <|-- Metrics\nDrift <|-- Metrics\nclass Metrics\n```\n### Canary and rollback (sequence)\n```mermaid\nsequenceDiagram\n  participant User\n  participant Router\n  participant Stable as StableModel\n  participant Canary as CanaryModel\n  participant Judge as Evaluator\n  participant Gate\n  User->>Router: request\n  Router->>Stable: 90% traffic\n  Router->>Canary: 10% traffic\n  Canary-->>Judge: output + trace\n  Judge-->>Gate: scores (quality, latency, cost)\n  Gate-->>Router: adjust split / rollback if thresholds violated\n```\n### Regression + canary guard (offline + online)\n```python\n# Offline: replay traces against new model and compare to goldens\nresults = replay_suite(traces, model=\"gpt-4o.2025-08\")\nmetrics = compute_metrics(results, goldens)\n\nassert metrics[\"success_rate\"] >= 0.98\nassert metrics[\"latency_p95_ms\"] <= 1.10 * baseline[\"latency_p95_ms\"]\nassert metrics[\"validator_fail_rate\"] <= baseline[\"validator_fail_rate\"]\n\n# Online: simple canary controller\nif window.qoQ_drop(\"success_rate\") > 2.0 or window.latency_p95_ms > SLO:\n    rollback(version=\"gpt-4o.2025-08\")\n    page_oncall(\"quality regression detected\")\n```\n### Micro-checklist\n- Version everything: prompts, models, tools, policies\n- Trace every step; redact at write-time; link to run_id\n- Maintain goldens; require regression gates before rollout\n- Canary with auto-rollback; keep a kill switch\n- Review metrics weekly; tune budgets, timeouts, and validators"
      },
      "speakerNotes": "Timing: 4:00 total\n\n0:00–0:25 — Set the philosophy\n- Say: We govern uncertainty with measurement.\n- Stage: Advance to Ops diagram slide.\n\n0:25–1:25 — Walk the Ops diagram\n- Point at flow: Run -> Trace -> Offline eval -> Metrics -> Gate -> Canary -> Full release.\n- Emphasize: Feedback loop and rollback path.\n- Prompt audience: \"How many of you have goldens today?\"\n\n1:25–2:00 — Tracing snippet\n- Technical reminder: Highlight span attributes (model, run_id, costs).\n- Note: Redaction at write-time for PII.\n- Stage: If live, briefly switch to terminal to show a sample JSON trace (optional).\n\n2:00–2:40 — Metrics taxonomy\n- Spend 10s each on effectiveness, efficiency, reliability, drift.\n- Call out actionable thresholds (p95, FPY).\n- Pause 3s to let the class diagram settle.\n\n2:40–3:20 — Canary sequence\n- Explain 90/10 split, evaluator scores, auto-adjust.\n- Mention kill switch and SLO breach.\n- Ask: \"What’s your rollback latency today?\"\n\n3:20–3:50 — Regression + canary guard code\n- Stress offline replay before any traffic.\n- Tie asserts to baseline metrics; show online guard.\n- Technical reminder: Mention feature flags/versions.\n\n3:50–4:00 — Close\n- Reiterate: instrument, evaluate, gate, then ship.\n- Transition: \"Next, we’ll look at case studies where this saved weeks of pain.\"",
      "narration": "We don’t eliminate uncertainty in multi‑agent systems; we instrument it... Think like an engineer: if you can observe it, you can govern it... In this segment we’ll connect the dots between evaluation, metrics, tracing, regression testing, and canary releases, so that non‑determinism becomes bounded and shippable.\n\nStart with the loop... A user request triggers orchestration... Every step—prompts, tool calls, costs, and latencies—lands in a trace... Those traces are not souvenirs; they are fuel... We replay them offline against new prompts or models, compute metrics, and compare to our golden expectations... A regression gate decides whether a change can graduate to a canary... If the canary holds under real traffic, we promote... If not, we roll back fast... That’s the rhythm: run, trace, evaluate, gate, canary, release, and always a path to safety.\n\nTo make this real, trace everything once and well... Create spans around agent runs and tool calls, attach attributes like model version, run ID, token counts, and cache hits, and redact sensitive fields at write time... Good traces let you answer hard questions: Why did p95 jump?... Which agent burned the budget?... Where did validation fail?... Without traces, you are arguing with anecdotes.\n\nWhat should we measure?... Start with four buckets... Effectiveness: task success rate and first‑pass yield—how often do we get to done without loops?... Efficiency: cost per task and p50 and p95 latency—because your customers feel tail latency more than averages... Reliability: tool success rate and validator failure rate—your guardrails are part of the system, so measure their friction and catches... And Drift: are embeddings shifting, are sources changing, are you seeing more prompt‑injection defenses fire?... These tell you when yesterday’s assumptions are quietly expiring.\n\nNow, ship safely with canaries... Route most traffic to the stable version and a small slice to the new one... Every canary response flows through an evaluator—deterministic checks first, like schema and policy validation, then calibrated LLM‑as‑judge with rubrics... Aggregate scores and watch your service‑level objectives: success rate, groundedness, latency, and cost... If quality degrades or p95 blows past the SLO, the gate adjusts the split or rolls back immediately... No heroics, just policy.\n\nAll of this depends on regression discipline... Before any canary, replay yesterday’s traces against today’s candidate... Compare to your goldens... Require that success rate meets or exceeds baseline, validator failures do not spike, and p95 latency stays within your budget... Treat models, prompts, and tools like any other dependency: pin versions, run the suite, record the deltas, and only then touch production traffic.\n\nA few pragmatic habits close the loop... Version everything—prompts, policies, tools, and models—and stamp those versions into every trace... Keep your goldens fresh; as domains evolve, so should acceptance criteria... Set explicit budgets and timeouts per agent, and enforce them in the orchestrator... And keep a literal kill switch... When the canary sings off‑key, you don’t negotiate; you roll back.\n\nPhilosophically, this is institutional design for intelligent systems... Traces are your history, metrics are your norms, evaluators are your judiciary, and canaries are your cautious experimentation... When you wrap emergence with contracts, feedback, and governance, you can move fast without breaking trust... That is how we operate like engineers and still make our customers’ dreams come true.",
      "duration": 4,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s10.tsx",
      "audioPath": "/audio/slide-s10.mp3"
    },
    {
      "id": "s11",
      "content": {
        "type": "markdown",
        "title": "Interactive: live poll (risk appetite) + Q&A lightning round (traps, budgets, loops)",
        "markdown": "- Live poll: What's your risk appetite today?\n  - Low: tight contracts, deterministic paths, human approvals\n  - Medium: guardrails + critics + canaries; ship within budgets\n  - High: explore emergence; recover with monitors and rollbacks\n  - Vote in the poll panel; we’ll tailor the Q&A based on results\n```mermaid\npie title Risk appetite (sample)\n  \"Low\" : 40\n  \"Medium\" : 45\n  \"High\" : 15\n```\n- Lightning Q&A themes (fast takes)\n  - Traps: prompt injection, ungrounded claims, hidden coupling\n  - Budgets: tokens, tool calls, wall-clock; per-agent caps\n  - Loops: step limits, heartbeats, similarity checks, circuit breakers\n- Budget governor (first principles)\n```python\nclass Budget:\n    def __init__(self, tokens=20000, tools=50, seconds=120):\n        self.tokens=tokens; self.tools=tools; self.deadline=time.time()+seconds\n    def allow_call(self, cost_tokens=0, tool=False):\n        if time.time()>self.deadline: return False, \"time\"\n        if self.tokens - cost_tokens < 0: return False, \"tokens\"\n        if tool and self.tools - 1 < 0: return False, \"tools\"\n        self.tokens -= cost_tokens; self.tools -= int(tool); return True, None\n```\n- Loop guard (stop the spin)\n```python\ndef should_stop(iter_i, last, curr, max_iter=8, sim_thr=0.97):\n    if iter_i >= max_iter: return True, \"max_iter\"\n    if heartbeat_missed(): return True, \"heartbeat\"\n    if cosine_sim(emb(last), emb(curr)) > sim_thr: return True, \"stagnation\"\n    return False, None\n```\n```mermaid\nflowchart TD\n  A[Task start] --> B{Exceeded limits?}\n  B -->|yes| X[Stop & escalate]\n  B -->|no| C[Execute agent]\n  C --> D[Validate: schema + rules + policy]\n  D -->|fail| E[Repair or backoff]\n  E --> F{Loop guard}\n  F -->|trip| X\n  F -->|safe| B\n  D -->|pass| G[Commit artifact]\n  G --> H{All goals met?}\n  H -->|yes| Y[Finish]\n  H -->|no| B\n```\n- Philosophical pointer\n  - We don’t eliminate uncertainty; we circumscribe it with contracts, critics, and budgets\n  - Your constitution > your framework; frameworks just make the constitution executable"
      },
      "speakerNotes": "- 0:00 — Introduce the interactive segment; set expectations that this will be fast-paced and hands-on.\n- 0:05 — Trigger the live poll in the webinar tool. Say: “Click the poll tab and choose Low, Medium, or High.”\n- 0:10 — Pause for 5 seconds to let people find the poll. Encourage: “If you can’t find it, drop your vote in chat.”\n- 0:15–0:40 — Let votes roll in; keep the pie chart visible. Share the philosophical line about circumscribing uncertainty while we wait.\n- 0:40 — Show results overlay. Respond: “I’m seeing X skew; great, we’ll tune answers accordingly.”\n- 0:45 — Transition to Lightning Q&A. Prompt: “Type ‘trap’, ‘budget’, or ‘loop’ in chat with your question.”\n- 0:50–2:20 — Run 3 quick questions, ~30–40s each.\n  - If chat is quiet, use pre-seeded Qs:\n    1) Trap: “How do we resist prompt injection?” Quick answer + point at contracts/validators.\n    2) Budget: “How do we avoid cost blowups?” Quick answer + point at Budget class.\n    3) Loop: “How do we stop infinite debates?” Quick answer + point at loop guard flowchart.\n- Stage directions:\n  - Keep the code snippet slide on screen during budgets/loops answers.\n  - Use the flowchart to illustrate the guard trip points.\n  - Call on one audience member by name if possible to increase engagement.\n- 2:20 — Summarize with the constitutional framing (contracts, critics, budgets).\n- 2:40 — Invite one last rapid question if time permits; otherwise, tee up the next section.\n- 2:55 — Close this segment and cue the next slide.\n- Tech reminders: ensure poll module is opened; have chat window visible; be ready to switch to code pane; watch the timer to keep total at 3:00.",
      "narration": "Let’s make this interactive... Quick pulse check: what’s your current risk appetite for shipping multi‑agent systems?... In the poll panel, choose Low, Medium, or High... Low means tight contracts and deterministic paths with human approvals... Medium means guardrails and critics with canaries... High means you’re exploring emergence and relying on monitors and rollbacks... Go ahead and vote now.\n\nWhile those votes come in, here’s the philosophical anchor: we don’t eliminate uncertainty; we circumscribe it... Orchestration is institutional design... We pick a constitution, a judiciary, and a budget office... Contracts define what agents may do... Critics interpret the rules... Budgets keep us honest... Emergence is welcome, but only within those boundaries.\n\nOkay, I’m seeing the results settle... Looks like there’s a tilt toward Medium, with some Lows and a brave handful of High... Perfect... Let’s do a lightning Q&A to match that spread... Drop your question with the word trap, budget, or loop.\n\nIf chat is warming up, I’ll seed one we always get: trap—how do we resist prompt injection?... Three quick moves... One, enforce instruction hierarchy in your prompts so system and tool policies can’t be overridden by user content... Two, sign or fingerprint retrieved context so the agent can verify integrity before acting... Three, gate high‑risk tools behind explicit justification plus a deterministic validator... Remember: the critic is your judiciary... Don’t let the agent self‑issue its own passport.\n\nBudget next... How do we avoid cost blowups?... Treat budget as first‑class state: tokens, tool calls, and wall‑clock... Give every agent a cap, and every run a cap... The tiny Budget class on the screen enforces allow or deny before each model or tool call... Pair that with concurrency limits and canaries, and you’ll know your spend before you hit deploy.\n\nLoops to close us out... How do we stop infinite debates or repair spirals?... You need three fail‑safes... One, a hard step limit... Two, a heartbeat watchdog that trips if progress stalls... And three, a similarity check between successive states: if we’re saying the same thing with different words, it’s time to stop or escalate... The flow on the right shows exactly where those breakers live—after validation, before we re‑enter the loop.\n\nBig idea to take with you: your constitution beats your framework... Contracts, critics, and budgets turn non‑determinism into governed exploration... With those in place, emergence becomes a feature, and you can make your customers’ dreams reliably come true... Alright, on to the next section.",
      "duration": 3,
      "transition": "fade",
      "componentPath": "./components/slides/Slide_s11.tsx",
      "audioPath": "/audio/slide-s11.mp3"
    },
    {
      "id": "s12",
      "content": {
        "type": "markdown",
        "title": "Closing: your constitution > your framework — concrete next steps and resources",
        "markdown": "- **Your constitution > your framework**\n  - Define success, non‑negotiables, and accountability first\n  - Contracts: agent I/O schemas, tool scopes, budgets, validators\n  - Governance beats knobs: iterate the rules, then swap frameworks\n- **Concrete next steps (7‑day sprint)**\n  - Day 1: Write the constitution (goals, hard no’s, guardrails)\n  - Day 2: Sketch a simple DAG; pick centralized orchestrator to start\n  - Day 3: Implement Planner + Critic first; keep agent contracts tiny\n  - Day 4–5: Add tracing, golden tests, policy validators\n  - Day 6–7: Canary a pilot; set kill switch; measure p50/p95, FPY\n```json\n{\n  \"success\": {\"acceptance\": \"goldens pass\", \"sla_ms\": 8000},\n  \"hard_nos\": [\"unguarded code exec\", \"PII exfil\"],\n  \"agents\": {\n    \"research\": {\"in\": \"Goal\", \"out\": \"Evidence[]\", \"tools\": [\"search_web\"], \"budget\": {\"tokens\": 2000}},\n    \"draft\": {\"in\": \"Outline+Evidence\", \"out\": \"Draft\", \"tools\": [\"extract_citations\"], \"budget\": {\"tokens\": 3000}}\n  },\n  \"validators\": [\n    {\"name\": \"schema\"},\n    {\"name\": \"policy\", \"rules\": [\"no-ungrounded-claims\", \"no-PII\"]},\n    {\"name\": \"citations\", \"type\": \"function\", \"args\": {\"coverage\": 1.0}}\n  ],\n  \"orchestration\": {\"topology\": \"centralized\", \"max_iterations\": 6, \"concurrency\": 4},\n  \"observability\": {\"trace\": true, \"redaction\": \"PII\", \"run_id\": \"uuid\"}\n}\n```\n```mermaid\nflowchart LR\n  C[\"Constitution\\n(contracts, policies, evals)\"] --> O[Orchestrator/DAG]\n  O --> P[Planner]\n  O --> A1[Agents]\n  O --> V[Validators/Critic]\n  A1 --> T[Tools]\n  V -->|accept/fix| O\n  O --> R[Result + Trace]\n```\n```mermaid\ngantt\n  title Week-1 Implementation Plan\n  dateFormat  X\n  section Define\n  Constitution Doc           :a1, 0, 1\n  section Build\n  Planner + Critic           :a2, 1, 2\n  Orchestrator Skeleton      :a3, 2, 1\n  Tracing + Goldens          :a4, 3, 2\n  section Operate\n  Canary + Kill Switch       :a5, 5, 2\n```\n- **Resources to keep you honest**\n  - Patterns: planner–executor, critique–refine, router–specialist, blackboard\n  - Eval: goldens, LLM-as-judge + deterministic checks, offline replay\n  - Ops & safety: OpenTelemetry-like traces, least-privilege tools, redaction\n  - Framework zone: pick any graph/DAG + function-calling + vector store + sandbox; version everything"
      },
      "speakerNotes": "- Spend ~20s: Frame the close — constitution over framework; norms beat knobs.\n- 30s: Walk the flowchart; point at Constitution feeding Orchestrator, Planner, Agents, Validators; emphasize feedback loop to accept/fix.\n- 35s: Step through the 7‑day plan; highlight Planner + Critic first, then tracing and goldens, then canary with a kill switch.\n- 20s: Show the JSON constitution snippet; call out hard_nos, budgets, validators.\n- 10s: Gantt timeline — reinforce that this is achievable in a week.\n- 5s: Quick interaction: Ask attendees to jot one “hard no” they’ll enforce tomorrow.\n- 10s: Resources bullets — patterns, eval, ops/safety, and the framework zone guidance.\n- Close: Thank audience; invite follow-up questions; remind them to version their constitution before swapping frameworks.\n- Technical reminders: Zoom into the mermaid flow; be ready to explain FPY (first‑pass yield); if asked, suggest starting with centralized orchestration before blackboard/marketplace.\n- Timing cue: Keep the narration under 2 minutes; avoid deep dives into specific tool brands.",
      "narration": "Let’s land this with the idea that will outlive any tool choice: your constitution matters more than your framework... We don’t tame uncertainty by tightening knobs; we govern it by declaring what success looks like, what’s non‑negotiable, and who is accountable... Contracts, policies, and evaluators are the norms your agent society lives by.\n\nHere’s the shape of it... The constitution feeds your orchestrator, the planner, the agents, and the critics... Validators close the loop, accepting or fixing before anything ships... Emergence is allowed, but only inside the fence you design.\n\nIf you want a concrete start, take a one‑week sprint... Day one, write the constitution: success criteria, hard no’s like unguarded code execution or PII exfiltration, and the budgets you will enforce... Day two, sketch a simple DAG and choose a centralized orchestrator so you can observe and control... Day three, build the planner and the critic first, and keep agent contracts tiny... Days four and five, add tracing, golden tests, and policy validators... Days six and seven, canary a pilot behind a kill switch, and measure latency percentiles and first‑pass yield.\n\nThe configuration can be explicit... Define agents with input and output schemas, tool scopes, and token budgets... Declare validators for schema, policy, and citations... Set max iterations and concurrency... Turn on tracing and redaction from day one... Version it all.\n\nFor resources, lean on a few core patterns: planner–executor, critique–refine, router–specialist, and blackboard when you need looser coupling... Evaluate with golden tasks and hybrid judges, and always replay traces offline before upgrades... Operate with end‑to‑end traces, least‑privilege tools, and prompt‑injection defenses.\n\nStart small, ship a pilot, and let your constitution evolve... Frameworks will change... Your norms, your markets, and your judiciary are what make your customers’ dreams reliably come true... Thank you.",
      "duration": 2,
      "transition": "fade",
      "voice": {
        "emotion": "friendly",
        "pace": "slow"
      },
      "componentPath": "./components/slides/Slide_s12.tsx",
      "audioPath": "/audio/slide-s12.mp3"
    }
  ],
  "transitions": {
    "default": "fade",
    "duration": 500
  }
}
